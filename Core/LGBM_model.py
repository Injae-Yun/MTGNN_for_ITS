import os
import numpy as np
import pandas as pd
import joblib
import glob
from dbfread import DBF
import psycopg2
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
from Utils.share import Scaler_tool
import h5py
import gc
from sklearn.model_selection import train_test_split
import torch

def prepare_feature_target_h5(
    ds: h5py.Dataset,
    emb_vals: np.ndarray,
    step: int,
    predict_steps: int,
    kind: str = 'x',
) -> np.ndarray:
    """
    ds:   HDF5 dataset (shape=(N, C, L) or (N, 1, L) for y)
    emb_vals: (L, E) node2vec embeddings
    step:     0 <= step < predict_steps
    predict_steps: e.g. 288
    kind: 'x' or 'y'
    """
    # 1) step ë‹¨ìœ„ë¡œ ìŠ¬ë¼ì´ìŠ¤ â†’ (D, C, L)
    arr = ds[step::predict_steps, ...]  
    D, C, L = arr.shape

    # 2) (D, C, L) â†’ (D, L, C) â†’ (D*L, C)
    X_flat = arr.transpose(0,2,1).reshape(D * L, C)

    if kind == 'y':
        # yëŠ” (D*L,) ìŠ¤ì¹¼ë¼ê°’
        return X_flat.ravel()

    # 3) emb_vals tile â†’ (D*L, E)
    E = emb_vals.shape[1]
    static_tile = np.tile(emb_vals, (D, 1))  # (D*L, E)

    # 4) concatenate â†’ (D*L, C+E)
    return np.hstack([X_flat, static_tile])
    
def log_to_logger(logger, period=1):
    """LightGBM í•™ìŠµ ë¡œê·¸ë¥¼ Python loggerë¡œ ë³´ë‚´ëŠ” ì½œë°±"""
    def _callback(env):
        if period > 0 and env.iteration % period == 0:
            result_str = f"[{env.iteration}] " + \
                         " ".join(f"{name}'s {metric}: {value:.5f}"
                                  for name, metric, value, _ in env.evaluation_result_list)
            logger.info(result_str)
    _callback.order = 10
    return _callback

def training_LGBM(
    link_ids,
    Config_path,
    output_dir_model,
    logger,
    emb_path: str = 'node2vec_embeddings.npy'
):
    predict_steps = 288  # í•˜ë£¨ 5ë¶„ ê°„ê²©

    # 1) ì„ë² ë”© ë¡œë“œ
    emb_vals = np.load(
        os.path.join(output_dir_model, emb_path),
        allow_pickle=True
    )  # shape = (L, E)

    # 2) HDF5ë¡œ ì €ì¥ëœ train/val/test/X, y ì—´ê¸°
    hf_x = h5py.File(os.path.join(output_dir_model, 'tensors.h5'), 'r')
    hf_y = h5py.File(os.path.join(output_dir_model, 'y_tensors.h5'),   'r')
    dsX = hf_x['train']
    dsY = hf_y['train']

    models = []
    model_names = []

    # 3) stepë³„ë¡œ í•™ìŠµ
    for step in range(predict_steps):
        fname = os.path.join(output_dir_model, f"lgbm_step_{step:03d}.pkl")
        if os.path.exists(fname):
            logger.info(f"âœ… Step {step} model already exists, skipping...")
            model_names.append(os.path.basename(fname))
            continue
        logger.info(f"â–¶ Training step {step+1}/{predict_steps}")

        # 3.1) ì´ ìŠ¤í…ì˜ X, y ì¤€ë¹„
        X = prepare_feature_target_h5(dsX, emb_vals, step, predict_steps, kind='x')
        y = prepare_feature_target_h5(dsY, emb_vals, step, predict_steps, kind='y')
        # 3.2) NaN ë§ˆìŠ¤í‚¹
        mask = ~np.isnan(y)
        X_clean = X[mask,:]
        y_clean= y[mask]
        X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)
        # 3.3) LightGBM í•™ìŠµ
        model = lgb.LGBMRegressor(
            objective='regression',
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
            num_leaves=63,
            max_depth=10,
            learning_rate=0.02,
            n_estimators=2500,
            # ê³¼ì í•© ë°©ì§€ ê·œì œ íŒŒëŒ
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha= 0.1,
            reg_lambda= 0.1,
            # ê¸°íƒ€ ì„¤ì •
            force_col_wise=True,
            random_state=42,
            verbose=1
        )
        model.fit(
            X_train_sub,
            y_train_sub,
            eval_set=[(X_val, y_val)],  # ê²€ì¦ ë°ì´í„°ì…‹ ì§€ì •
            eval_names=['valid'],            # ì´ë¦„ ëª…ì‹œ
            eval_metric='rmse',         # í‰ê°€ ì§€í‘œ
            callbacks=[
                lgb.early_stopping(stopping_rounds=50),
                log_to_logger(logger, period=100)  # 50 ë¼ìš´ë“œ ë§ˆë‹¤ loggerë¡œ ì¶œë ¥
            ]
        )
        # âœ… í•™ìŠµ í›„ ë³„ë„ë¡œ í‰ê°€
        train_preds = model.predict(X_train_sub)
        train_rmse = mean_squared_error(y_train_sub, train_preds, squared=False)
        valid_rmse = model.best_score_['valid']['rmse']

        logger.info(
            f"ğŸ† Step {step} Best iteration: {model.best_iteration_}, "
            f"Train RMSE: {train_rmse:.5f}, Valid RMSE: {valid_rmse:.5f}"
        )
        # 3.4) ì €ì¥
        joblib.dump(model, fname)
        logger.info(f"âœ” Saved model step {step} â†’ {fname}")

        #models.append(model)
        model_names.append(os.path.basename(fname))

        # ë©”ëª¨ë¦¬ í•´ì œ
        del X, y, model
        gc.collect()

    # 4) ë§ˆë¬´ë¦¬
    hf_x.close()
    hf_y.close()

    np.save(
        os.path.join(output_dir_model, "lgbm_model_names.npy"),
        np.array(model_names, dtype='<U32')
    )
    logger.info(f"âœ… Training complete: {len(model_names)} models saved")
    return model_names

# ì¶”ë¡  ë‹¨ê³„

def predict_LGBM(
    models_dir: str,
    model_names: list[str],
    D: int,
    L: int,
    logger,
    emb_path: str = 'node2vec_embeddings.npy',
    mode = "train"
) -> np.ndarray:
    predict_steps = 288
    # 1) ì„ë² ë”© & ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    emb_vals = np.load(os.path.join(models_dir, emb_path), allow_pickle=True)
    scaler = Scaler_tool()
    #scaler = joblib.load(os.path.join(models_dir, "scaler.npy"))  # ìˆ˜ì •: scalerë„ joblibë¡œ ì €ì¥í–ˆë‹¤ê³  ê°€ì •

    # 2) HDF5 í…ŒìŠ¤íŠ¸ì…‹ ì—´ê¸°

    if mode =='train':
        hf_x = h5py.File(os.path.join(models_dir, 'tensors.h5'), 'r')
        hf_y = h5py.File(os.path.join(models_dir, 'y_tensors.h5'),   'r')
        dsX_test = hf_x['test']
        dsY_test = hf_y['test']
        D, L = dsX_test.shape[0] // predict_steps, dsX_test.shape[2]
        all_pred_1d = np.empty((predict_steps, D*L), dtype=np.float32)
        all_true_1d = np.empty((predict_steps, D*L), dtype=np.float32)
            # 3) stepë³„ ì˜ˆì¸¡ & ìŠ¤í…ë³„ ì§€í‘œ
        for step, model_fname in enumerate(model_names):
            model = joblib.load(os.path.join(models_dir, model_fname))

            # 3.1) ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ X, y ë¶ˆëŸ¬ì˜¤ê¸°
            X = prepare_feature_target_h5(dsX_test, emb_vals, step, predict_steps, kind='x')
            y= prepare_feature_target_h5(dsY_test, emb_vals, step, predict_steps, kind='y')

            # 3.2) ì˜ˆì¸¡
            y_pred = model.predict(X)
            # 3.3) NaN ë§ˆìŠ¤í¬
            mask = ~np.isnan(y)
            y_clean_1d = y[mask]
            y_pred_1d = y_pred[mask]
            all_pred_1d[step, :] = y_pred_1d
            all_true_1d[step, :] = y_clean_1d
            # 3.5) ìŠ¤í…ë³„ ì§€í‘œ ê³„ì‚°
            mse  = mean_squared_error(y_clean_1d, y_pred_1d)
            rmse = np.sqrt(mse)
            mae  = mean_absolute_error(y_clean_1d, y_pred_1d)
            r2   = r2_score(y_clean_1d, y_pred_1d)
            y_pred_1d = scaler.inverse_transform(y_pred_1d)
            y_clean_1d = scaler.inverse_transform(y_clean_1d)

            y_clean_1d [y_clean_1d< 5] = 5  # ìµœì†Œê°’ ì„¤ì • #divided by 0 ë°©ì§€
            mape = mean_absolute_percentage_error(y_clean_1d, y_pred_1d)

            logger.info(
                f"Step {step:03d} â†’ RMSE={rmse:.3f}, MAE={mae:.3f}, "
                f"MAPE={mape:.4f}, R2={r2:.3f}"
            )
            gc.collect()

        hf_x.close()
        hf_y.close()
        all_true_1d = all_true_1d.reshape(-1)
        all_pred_1d = all_pred_1d.reshape(-1)
        mask = ~np.isnan(all_true_1d)
        all_true_1d = all_true_1d[mask]
        all_pred_1d = all_pred_1d[mask]
        # 4) ì „ì²´ ì§€í‘œ
        mse  = mean_squared_error(all_true_1d, all_pred_1d)
        tot_rmse = np.sqrt(mse)
        tot_mae  = mean_absolute_error(all_true_1d, all_pred_1d)
        tot_r2   = r2_score(all_true_1d, all_pred_1d)
        # ì „ì²´ MAPE ê³„ì‚°
        all_true_1d = scaler.inverse_transform(all_true_1d)
        all_pred_1d = scaler.inverse_transform(all_pred_1d)
        tot_mape = mean_absolute_percentage_error(all_true_1d, all_pred_1d)

        logger.info(
            f"Overall â†’ RMSE={tot_rmse:.3f}, MAE={tot_mae:.3f}, "
            f"MAPE={tot_mape:.4f}, R2={tot_r2:.3f}"
        )
    else: 
        hf_x = h5py.File(os.path.join(models_dir, 'predict_tensors.h5'), 'r')
        dsX_test = hf_x['predict']
        D, L = dsX_test.shape[0] // predict_steps, dsX_test.shape[2]
        all_pred_2d = np.empty((predict_steps, D, L), dtype=np.float32)
        # 3) stepë³„ ì˜ˆì¸¡ & ìŠ¤í…ë³„ ì§€í‘œ
        for step, model_fname in enumerate(model_names):
            model = joblib.load(os.path.join(models_dir, model_fname))
            # 3.1) ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ X, y ë¶ˆëŸ¬ì˜¤ê¸°
            X = prepare_feature_target_h5(dsX_test, emb_vals, step, predict_steps, kind='x')
            y_pred = model.predict(X)
            # 3.2) ì—­ìŠ¤ì¼€ì¼ë§
            y_pred = scaler.inverse_transform(y_pred)
            # 3.4) 1d -> 2d ì¬ë³€í™˜
            all_pred_2d[step, :, :] = y_pred.reshape(D,L)

        hf_x.close()
        logger.info('Predicting complete, returning predictions')
        
    # 5) ë§ˆì§€ë§‰ì— (N_test, 288) í˜•íƒœë¡œ ë°˜í™˜
    # all_preds ë¦¬ìŠ¤íŠ¸ëŠ” [step0_preds, step1_preds, â€¦], ê° ê¸¸ì´ê°€ N_test_days*L
    return all_pred_2d# (steps_288, N_test_days*L) í˜•íƒœë¡œ ë°˜í™˜

