import subprocess, json, os, gc
import pandas as pd
import fastparquet
from pymongo import MongoClient
from datetime import datetime, timedelta
from subprocess import Popen, PIPE
import multiprocessing
from pymongo.errors import AutoReconnect
import time
import pyarrow as pa
import pyarrow.parquet as pq
import psycopg2
from psycopg2.extras import execute_values
from psycopg2 import OperationalError
from datetime import datetime, timedelta

class Postgres:
    #
    def __init__(self,db_config):
        self.db_config = db_config
        self._connect()

    def _connect(self):
        """ìƒˆë¡œìš´ ì»¤ë„¥ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        # ê¸°ì¡´ ì»¤ë„¥ì…˜ì´ ìˆìœ¼ë©´ ë‹«ê³ 
        try:
            self.conn.close()
        except Exception:
            pass
        # ë‹¤ì‹œ ì—°ê²°
        self.conn = psycopg2.connect(
            host     = self.db_config['host'],
            port     = self.db_config['port'],
            database = self.db_config['database'],
            user     = self.db_config['user'],
            password = self.db_config['password'],
        )
        self.conn.autocommit = False

    # info.link
    # orgin data
    def select_links(self,
        date: datetime,
        link_ids: list[str],        
    ):
        cursor = self.conn.cursor()
        table_date = date.strftime("%y%m")
        start_date = date.strftime("%Y-%m-%d 00:00:00")
        end_date = (date + timedelta(days=1)).strftime("%Y-%m-%d 00:00:00")
        # 1) ì„ì‹œ í…Œì´ë¸” ìƒì„±
        cursor.execute("DROP TABLE IF EXISTS temp_links")
        cursor.execute("CREATE TEMP TABLE temp_links(link_id TEXT PRIMARY KEY)")
        
        # 2) ì„ì‹œ í…Œì´ë¸”ì— link_ids ì‚½ì… (bulk copy ì‚¬ìš©)
        psycopg2.extras.execute_values(
            cursor,
            "INSERT INTO temp_links (link_id) VALUES %s",
            [(lid,) for lid in link_ids],
            page_size=10000  # ì ë‹¹í•œ batch size ì„¤ì •
        )
        # 3) JOINì„ í™œìš©í•œ ì¿¼ë¦¬ ìˆ˜í–‰
        query = f"""
            SELECT il.*
            FROM info.link_{table_date} il
            JOIN temp_links tl ON il.link_id = tl.link_id
            WHERE il.reg_date >= %s AND il.reg_date < %s
        """
        cursor.execute(query, (start_date, end_date))

        results = cursor.fetchall()
        
        self.conn.commit()
        cursor.close()

        return results


def archive_to_parquet(
    archive_path,
    parquet_path,
    projection=None,
    logger=None
):
    # ë³‘ë ¬ í•´ì œë¥¼ ìœ„í•´ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ê²°ì •
    n_threads = multiprocessing.cpu_count()

    # 1) pigz -dc -p N archive_path  |  bsondump
    p1 = Popen(
        ["pigz", "-dc", "-p", str(n_threads), archive_path],
        stdout=PIPE
    )
    p2 = Popen(
        ["bsondump"], 
        stdin=p1.stdout, 
        stdout=PIPE, 
        text=True
    )
    p1.stdout.close()  # íŒŒì´í”„ ì—°ê²°

    # 2) JSON â†’ ë¦¬ìŠ¤íŠ¸
    docs = []
    for line in p2.stdout:
        obj = json.loads(line)
        if projection:
            obj = {k: obj.get(k) for k,v in projection.items() if v==1}
        docs.append(obj)
    p2.stdout.close()
    p2.wait()
    p1.wait()

    # ì´í›„ df â†’ fastparquet.write(...) ë™ì¼
    df = pd.DataFrame(docs)
    if projection:
        cols = [k for k,v in projection.items() if v==1]
        df = df[cols]
    fastparquet.write(parquet_path, df,
                      compression="SNAPPY",
                      write_index=False)
    if logger:
        logger.info(f"âœ… BSONâ†’Parquet ì™„ë£Œ: {parquet_path} ({len(docs)}ê±´)")

def load_from_mongo(link_ids, dates, time_points,
                    projection, mongo_config,
                    output_dir, target, logger,
                    batch_size=10000, parquet_chunk=20000):
    os.makedirs(output_dir, exist_ok=True)
    client_uri = f"mongodb://{mongo_config['host']}:{mongo_config['port']}/{mongo_config['db']}"
    logger.info("ğŸš€ ì‹œì‘ â€“ MongoDB â†’ Parquet ìŠ¤íŠ¸ë¦¬ë°")

    # time_points map: { 'YYYY-MM-DD': ['HH:MM', ...] }
    date_time_map = {}
    for tp in time_points or []:
        dt = tp if isinstance(tp, datetime) else datetime.strptime(tp, "%Y-%m-%d %H:%M")
        key = dt.strftime("%Y-%m-%d")
        date_time_map.setdefault(key, []).append(dt.strftime("%H:%M"))

    # link_id íƒ€ì… í˜¼í•© ë§¤ì¹­ ëŒ€ë¹„ (str/int ë™ì‹œ í¬í•¨)
    # - DB í•„ë“œê°€ intì¸ë° ë¦¬ìŠ¤íŠ¸ê°€ strì´ë©´ ë¯¸ìŠ¤ë§¤ì¹˜ ë°œìƒ â†’ ë‘˜ ë‹¤ ë„£ì–´ ë§¤ì¹­
    link_ids = link_ids.tolist()
    link_ids_mixed = []
    for lid in link_ids:
        link_ids_mixed.append(lid)
        try:
            link_ids_mixed.append(int(lid))
        except Exception:
            pass
    # ì¤‘ë³µ ì œê±°
    try:
        from collections import OrderedDict
        link_ids_mixed = list(OrderedDict((x, None) for x in link_ids_mixed).keys())
    except Exception:
        link_ids_mixed = list(dict.fromkeys(link_ids_mixed))
    results = []
    cilent = MongoClient(mongo_config["host"], mongo_config["port"],
                serverSelectionTimeoutMS=60000,  # ì„œë²„ ì…€ë ‰ì…˜ 60ì´ˆ
                socketTimeoutMS=120000,          # ì½ê¸° íƒ€ì„ì•„ì›ƒ 120ì´ˆ
                connectTimeoutMS=20000           # ì—°ê²° ì‹œë„ íƒ€ì„ì•„ì›ƒ 20ì´ˆ                      
                )
#        logger.info(cilent)
#        logger.info(mongo_config)

    db = cilent[mongo_config["db"]]
    # ì²­í¬ í¬ê¸°: ëŒ€ì—­í­/ì»¬ë ‰ì…˜ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì ˆ
    interval = timedelta(hours=3) if target == 'Korea' else timedelta(hours=24)

    for date in dates:
        coll = db[f"traffic_linkdata_{date}"]
        logger.info(f"ğŸ“¡ [{date}] ì»¬ë ‰ì…˜ ì—°ê²°")
        # build query
        iso = f"{date[:4]}-{date[4:6]}-{date[6:]}"
        # ê¸°ë³¸ ë§í¬ í•„í„°
        query = {"link_id": {"$in": link_ids_mixed}}
        # í•„ìš” ì‹œì ì´ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´ $in ìœ¼ë¡œë§Œ ì œí•œ (ë¶ˆí•„ìš”í•œ range ê²°í•© ì œê±°)
        #   - ì¼ë¶€ í™˜ê²½ì—ì„œ insert_time ì €ì¥ í¬ë§·ì´ HH:MM:SSì¸ ê²½ìš°ë„ ìˆì–´
        #     rangeë§Œ ë‚¨ê²¨ë‘ë©´ ì „ì²´ì¼ ì¡°íšŒê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ. (ì•„ë˜ì—ì„œ ë³´ì™„)
        time_list = date_time_map.get(iso, [])
        if time_list:
            query["insert_time"] = {"$in": time_list}
        # prepare output
        out_path = os.path.join(output_dir, f"{date}.parquet")
        buffer, written = [], False
        all_docs = []
        # 00:00ë¶€í„° ë‹¤ìŒë‚  00:00 ì§ì „ê¹Œì§€ 6ì‹œê°„ì”© ìª¼ê°œê¸°
        day_start = datetime.strptime(iso, "%Y-%m-%d")
        chunk_start = day_start
        day_end = day_start + timedelta(days=1)-timedelta(minutes=1) # 23:59

        while chunk_start < day_end:
            chunk_end = min(chunk_start + interval, day_end)
            time_gte = chunk_start.strftime("%H:%M")
            time_lt  = chunk_end.strftime("%H:%M")

            # ì´ êµ¬ê°„ ì „ìš© ì¿¼ë¦¬ êµ¬ì„±
            q = query.copy()
            if time_list:
                # ì´ ì²­í¬ì— í•´ë‹¹í•˜ëŠ” ë¶„ ë‹¨ìœ„ íƒ€ì„ë§Œ ì¶”ë¦¬ê¸°
                times_in_chunk = [t for t in time_list if (t >= time_gte and t < time_lt)]
                if not times_in_chunk:
                    chunk_start = chunk_end
                    continue  # ì´ êµ¬ê°„ì—” ì¡°íšŒí•  ë¶„ì´ ì—†ìŒ
                # HH:MM ì™€ HH:MM:00 ë‘ í˜•ì‹ì„ ëª¨ë‘ í¬í•¨í•˜ì—¬ ì •í™• ë§¤ì¹­ ($in)ë§Œ ì‚¬ìš©
                times_exact = list(dict.fromkeys(
                    times_in_chunk + [t + ":00" for t in times_in_chunk]
                ))
                q["insert_time"] = {"$in": times_exact}
            else:
                # date_range ë“±: range í•„í„° ì ìš©
                time_filter = {"insert_time": {"$gte": time_gte, "$lt": time_lt}}
                if q:
                    q = {"$and": [q, time_filter]}
                else:
                    q = time_filter

            # retry ë¡œì§ìœ¼ë¡œ ì´ êµ¬ê°„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            retry = 0
            max_retry = 12
            temp = None
            t0 = time.time()
            while retry < max_retry:
                try:
                    cursor = coll.find(q, projection, batch_size=batch_size)
                    temp = []
                    for doc in cursor:
                        temp.append(doc)
                    # ì„±ê³µì ìœ¼ë¡œ ë‹¤ ì½ìœ¼ë©´ break
                    break
                except AutoReconnect as e:
                    logger.warning(f"âš ï¸ AutoReconnect: {e}. ì¬ì‹œë„ {retry+1}/{max_retry}")
                    retry += 1
                    time.sleep(5)
                finally:
                    try: cursor.close()
                    except: pass

            if temp is None:
                logger.error(f"âŒ [{date} {time_gte}-{time_lt}] ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼, ìŠ¤í‚µ")
            elif not temp:
                logger.info(f"â„¹ï¸ [{date} {time_gte}-{time_lt}] ë¬¸ì„œ ì—†ìŒ")
            else:
                all_docs.extend(temp)
                dt_sec = time.time() - t0
                logger.info(f"âœ… [{date} {time_gte}-{time_lt}] {len(temp)}ê±´ ì¡°íšŒ (%.2fs)" % dt_sec)

            # ë‹¤ìŒ 6ì‹œê°„ìœ¼ë¡œ
            chunk_start = chunk_end

        # í•˜ë£¨ ì „ì²´ ëª¨ì€ ë’¤ Parquet ì €ì¥
        if not all_docs:
            logger.warning(f"âš ï¸ [{date}] ì „ì²´ ë°ì´í„° ì—†ìŒ, ë‹¤ìŒ ë‚ ì§œë¡œ")
            continue
        # DataFrame ìƒì„±
        df = pd.DataFrame(all_docs)

        # projectionì´ ìˆë‹¤ë©´ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        if projection:
            cols = [k for k, v in projection.items() if v == 1]
            df = df[cols]

        # Parquetìœ¼ë¡œ í•œ ë²ˆì— ì €ì¥
        t0 = time.time()
        fastparquet.write(
            out_path, 
            df, 
            compression="SNAPPY", 
            write_index=False
        )
        logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {out_path} rows={len(df)} (%.2fs)" % (time.time() - t0))

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del  all_docs, df
        gc.collect()

    logger.info("ğŸ“¦ ëª¨ë“  ë‚ ì§œ Parquet ë³€í™˜ ì™„ë£Œ")


def load_from_postgres(link_ids, dates, time_points,
                    Task_config, db_config,
                    output_dir, target, logger,
                    parquet_chunk=25000):
    os.makedirs(output_dir, exist_ok=True)
    pg = Postgres(db_config)
    logger.info("ğŸš€ ì‹œì‘ â€“ Postgres DB â†’ Parquet ìŠ¤íŠ¸ë¦¬ë°")

    # time_points map
    date_time_map = {}
    for tp in time_points or []:
        dt = tp if isinstance(tp, datetime) else datetime.strptime(tp, "%Y-%m-%d %H:%M")
        key = dt.strftime("%Y-%m-%d")
        date_time_map.setdefault(key, []).append(dt.strftime("%H:%M"))
    link_ids = link_ids.tolist()

    # if target == 'Korea':
    #     interval = timedelta(hours=3)  # 6ì‹œê°„ ê°„ê²©
    # else:
    #     interval = timedelta(hours=24)
    max_retries =5
    backoff_sec= 5.0
    counts = 0

    interest = Task_config['far_time_process']["interest"]   # e.g. [-21,...,0]
    offsets  = interest[:]
    interval = Task_config['interval']  # in minutes, e.g. 5
    for date in dates:        
        if type(date) != str:
            date_dt = date.strftime("%Y%m%d")
        else:
            date_dt = datetime.strptime(date,"%Y%m%d")
        for ci, off in enumerate(offsets):
            d = date_dt + timedelta(days=off)
            fn = os.path.join(output_dir, d.strftime("%Y%m%d") + ".parquet")
        # 1) ì´ë¯¸ íŒŒì¼ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ
            if os.path.exists(fn):
                logger.info(f"[{d}] ì´ë¯¸ ì²˜ë¦¬ëœ ë‚ ì§œ, ìŠ¤í‚µ")
                continue
            writer = None
            try:
                for i in range(0, len(link_ids), parquet_chunk):
                    chunk = link_ids[i:i+parquet_chunk]

                    all_docs = pg.select_links(d, chunk)
                    # --- retry wrapper ---
                    for attempt in range(max_retries):
                        try:
                            all_docs = pg.select_links(d, chunk)
                            break
                        except OperationalError as e:
                            msg = str(e)
                            if "SSL SYSCALL error: EOF detected" in msg and attempt < max_retries - 1:
                                logger.warning(
                                    f"[{d:%Y-%m-%d}] SSL EOF, reconnecting and retrying "
                                    f"(attempt {attempt+1}/{max_retries})"
                                )
                                pg._connect()               # ì»¤ë„¥ì…˜ ì¬ìƒì„±
                                time.sleep(backoff_sec)     # backoff
                                continue
                            else:
                                logger.error(
                                    f"[{date:%Y-%m-%d}] select_links failed: {e!r}"
                                )
                                raise
                    else:
                        # retry ë£¨í”„ë¥¼ break ì—†ì´ ë¹ ì ¸ë‚˜ì™”ë‹¤ë©´ ì‹¤íŒ¨
                        raise RuntimeError(f"Failed after {max_retries} retries")
                    
                    # í•˜ë£¨ ì „ì²´ ëª¨ì€ ë’¤ Parquet ì €ì¥
                    if not all_docs:
                        logger.warning(f"âš ï¸ [{date}] ì „ì²´ ë°ì´í„° ì—†ìŒ, ë‹¤ìŒ ë‚ ì§œë¡œ")
                        df, df_final, table =[], [],[]
                        continue
                    # DataFrame ìƒì„±
                    cols = db_config["tables"]["info.link"]["columns"]
                    df = pd.DataFrame(all_docs, columns=cols)

                    # projectionì´ ìˆë‹¤ë©´ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
                    projection = Task_config['far_time_process'].get('pg_projection', None)
                    explode_list = Task_config['far_time_process'].get('explode_list', None)
                    if projection:
                        cols = [k for k, v in projection.items() if v == 1]
                        df = df[cols]
                    df_exp = df.explode('data').reset_index(drop=True)
                    data_expanded = pd.json_normalize(df_exp['data'])
                    if explode_list:
                        # ì•ˆì „í•˜ê²Œ, ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°
                        valid_cols = [c for c in explode_list if c in data_expanded.columns]
                        data_expanded = data_expanded[valid_cols]
                    df_final = pd.concat([
                        df_exp.drop(columns=['data']),
                        data_expanded
                    ], axis=1)
                    # 1) speed, pty ì»¬ëŸ¼ì„ pandas nullable Int64ë¡œ ë³€í™˜ (NaN í—ˆìš©)
                    for col in ["speed", "pty"]:
                        if col in df_final.columns:
                            # NaNì„ -1 ë“± íŠ¹ì • ê°’ìœ¼ë¡œ ëŒ€ì²´í•  ê±°ë©´ fillna(-1) í›„ astype('int64') í•´ë„ ë©ë‹ˆë‹¤.
                            df_final[col] = df_final[col].round().astype(pd.Int64Dtype())
                    table = pa.Table.from_pandas(df_final, preserve_index=False)

                    # 4) ParquetWriter ìƒì„±(ì²« ë¸”ë¡ì—ë§Œ)
                    if writer is None:
                        writer = pq.ParquetWriter(
                            fn,
                            schema=table.schema,
                            compression='snappy'
                        )

                    # 5) RowGroup ë‹¨ìœ„ë¡œ append
                    writer.write_table(table)
                    logger.info(f"[{d:%Y-%m-%d}] chunk {i}-{i+parquet_chunk}: write_table")
                    # chunk ë£¨í”„ ëë‚˜ë©´ ë°˜ë“œì‹œ close
                    del  all_docs, df_final, table
                    gc.collect()

            except Exception as e:
                # 4) ì—ëŸ¬ ë°œìƒ ì‹œ, ì´ë¯¸ ë§Œë“¤ì–´ì§„ íŒŒì¼ ì‚­ì œ
                if writer:
                    writer.close()
                if os.path.exists(fn):
                    os.remove(fn)
                    logger.warning(f"[{d}] ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ, ì„ì‹œíŒŒì¼ ì‚­ì œ: {fn}")
                raise  # ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë˜ì ¸ì„œ í˜¸ì¶œë¶€ì—ì„œ ì•Œ ìˆ˜ ìˆê²Œ

            else:
                # ì •ìƒ ì™„ë£Œ ì‹œ
                if writer:
                    writer.close()
                    logger.info(f"[{d}] parquet file completed: {d}")
                else:
                    logger.warning(f"[{d}] ì „ì²´ ë°ì´í„° ì—†ìŒ, parquet íŒŒì¼ ë¯¸ìƒì„±")
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()

    logger.info("ğŸ“¦ ëª¨ë“  ë‚ ì§œ Parquet ë³€í™˜ ì™„ë£Œ")