import pandas as pd
import os
import numpy as np
from datetime import timedelta, datetime
import holidays
from multiprocessing import Pool, cpu_count
from Utils.share import Scaler_tool
from dbfread import DBF

from datetime import datetime, timedelta
import os
import pandas as pd
import gc
import pyarrow as pa
import pyarrow.parquet as pq
import h5py

import os, gc
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

import multiprocessing as mp
from functools import partial

def process_day_to_linkwise(df, date, processed_dir, full_time_index,logger):
    df["insert_time"] = pd.to_datetime(df["insert_time"], format="%H:%M").dt.time
    grouped = df.groupby("link_id")
    timestamp = logger.timestamp  # ë¡œê±°ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ê°€ì ¸ì˜¤ê¸°
    for link_id, group in grouped:
        group = group[["insert_time", "speed"]]
        group.set_index("insert_time", inplace=True)
        group = group.reindex(full_time_index)  # NaN ì±„ì›€
        group["speed"] = group["speed"].astype("float32")
        # date_only = timestamp[:10]
        # save_path = os.path.join(processed_dir,date_only, f"{link_id}.parquet")
        save_path = os.path.join(processed_dir, f"{link_id}.parquet")
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        # ë‚ ì§œ ì»¬ëŸ¼ìœ¼ë¡œ êµ¬ë¶„ í›„ append
        group = group.rename_axis("time").reset_index()
        group["date"] = date

        if os.path.exists(save_path):
            old_df = pd.read_parquet(save_path) #2d format ìµœì 
            merged = pd.concat([old_df, group], ignore_index=True)
        else:
            merged = group

        merged.to_parquet(save_path, index=False)
    logger.info(f"âœ… ë‚ ì§œ {date}: ì •ìƒ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤..")
    gc.collect()  # ë©”ëª¨ë¦¬ ì •ë¦¬

    
def load_tensor_for_link(
    link_id,
    base_start: pd.Timestamp,
    offsets: list[int],    # ë¶„ ë‹¨ìœ„ ì˜¤í”„ì…‹ ë¦¬ìŠ¤íŠ¸
    max_offset: int,       # ë¶„ ë‹¨ìœ„ ìµœëŒ€ ì˜¤í”„ì…‹
    selectedLink: pd.DataFrame,
    processed_data_dir: str,
    type: str,
    Processmode:str,
    logger
):
    # parquet ê²½ë¡œ
    file_path = os.path.join(processed_data_dir, f"{link_id}.parquet")

    # max_speed ì¡°íšŒ
    max_speed = float(
        selectedLink.loc[
            selectedLink['LINK_ID']==link_id,
            'MAX_SPD'
        ].iat[0]
    )

    # ì „ì²´ ê°€ëŠ¥í•œ base_time ìƒì„± (íŒŒì¼ì´ ìˆë“  ì—†ë“  ë™ì¼)
    # end_time = í•˜ë£¨ ë’¤ 00:00 ì§ì „ì—ì„œ max_offset ë¶„ ëº€ ì‹œì 
    if Processmode == 'date_range':
        end_time = base_start + timedelta(days=1) - timedelta(minutes=max_offset)
    elif Processmode == 'time_reference':
        end_time = base_start + timedelta(minutes=max_offset)
    # offsets ê°„ê²©: offsets ê°€ [0,5,10,...] ì‹ì´ì–´ì•¼ í•¨
    if len(offsets) > 1:
        step = offsets[1] - offsets[0]
    else:
        step = max_offset  # fallback
    # 00:00 ë¶€í„° end_time ê¹Œì§€ step ë¶„ ê°„ê²©
    all_base_times = pd.date_range(
        start=base_start, end=end_time, freq=f"{step}T"
    )

    # íŒŒì¼ ì—†ìœ¼ë©´ NaN ì±„ìš´ ë°°ì—´ ë°˜í™˜
    if not os.path.exists(file_path):
        if logger:
            logger.warning(f"íŒŒì¼ ì—†ìŒ, NaN tensorë¡œ ëŒ€ì²´: {file_path}")
        # (T, len(offsets))
        nan_tensor = np.full(
            (len(all_base_times), len(offsets)),
            np.nan,
            dtype=float
        )
        return nan_tensor, link_id, all_base_times

    # ì‹¤ì œ íŒŒì¼ì´ ìˆìœ¼ë©´ ì›ë˜ ë¡œì§
    df = pd.read_parquet(file_path, engine="pyarrow")
    df["datetime"] = pd.to_datetime(
        df["date"].astype(str) + " " + df["time"].astype(str)
    )
    df = df.set_index("datetime")["speed"].sort_index()

    # ìœ íš¨í•œ base_times í•„í„°
    valid = (all_base_times >= base_start) & (all_base_times <= df.index[-1] - timedelta(minutes=max_offset))
    base_times = all_base_times[valid]

    link_tensor = []
    for bt in base_times:
        time_points = [bt + timedelta(minutes=o) for o in offsets]
        row = df.reindex(time_points).values  # shape = (len(offsets),)
        if type == 'x':
            # NaN â†’ max_speed
            row = np.where(np.isnan(row), max_speed, row)
        link_tensor.append(row)

    if len(link_tensor) == 0:
        # ìœ íš¨í•œ ì‹œê°„ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´, NaN tensorë¡œ
        nan_tensor = np.full(
            (len(all_base_times), len(offsets)),
            np.nan,
            dtype=float
        )
        return nan_tensor, link_id, all_base_times

    return np.stack(link_tensor), link_id, base_times

def build_cluster_tensor(clusters, processed_data_dir, path_config, task_config, output_dir, logger,mode='train',cluster_id=None):
    """
    í´ëŸ¬ìŠ¤í„° ë‹¨ìœ„ë¡œ (T, 1, N_links, 36) í˜•íƒœì˜ í…ì„œë¥¼ ìƒì„±í•˜ì—¬ ì €ì¥

    Args:
        cluster_path (str): LivingLab_clusters.pkl ê²½ë¡œ
        processed_data_dir (str): ë§í¬ë³„ parquet íŒŒì¼ ì €ì¥ í´ë”
        offsets (list[int]): ê´€ì‹¬ ì‹œê°„ ì˜¤í”„ì…‹ (ì˜ˆ: [-60, -55, ..., -5])
        output_dir (str): í…ì„œ ì €ì¥ ê²½ë¡œ
        logger (Logger, optional): ë¡œê¹… ê°ì²´
    """
    os.makedirs(output_dir, exist_ok=True)
    ITS_Link_path = path_config['data']['ITS_info']
    df = DBF(ITS_Link_path, encoding='cp949')
    Linkdata = pd.DataFrame(iter(df))
    selectedLink = Linkdata[['LINK_ID', 'MAX_SPD']]
    timestamp = logger.timestamp  # ë¡œê±°ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ê°€ì ¸ì˜¤ê¸°
    date_only = timestamp[:10]  # '2025-06-27' í˜•ì‹ìœ¼ë¡œ ë‚ ì§œë§Œ ì¶”ì¶œ
    interval=task_config['interval']  # ì‹œê°„ ê°„ê²© (ë¶„ ë‹¨ìœ„)
    interest = task_config['interest']
    interest_y = task_config.get('interest_y',0)
    train_ratio=task_config['train_ratio']
    Processmode =task_config['mode']
    offsets = []
    for base in interest:
        offsets.extend([interval * i for i in range(base, base + 12)])  # 12ê°œì”©
    offsets = sorted(offsets)  # ì‹œê°„ ìˆœ ì •ë ¬

    # ê°€ëŠ¥í•œ base time (ê°€ì¥ ë¹ ë¥¸ offset ê¸°ì¤€ í™•ë³´ëœ ì‹œê°„ ë²”ìœ„)
    max_offset = interval *interest_y
    if Processmode == 'date_range':
        start_date = datetime.strptime(task_config['date_range']['start'], "%Y-%m-%d")
        base_start = start_date
    elif Processmode == 'time_reference':
        time_ref = task_config["time_reference"]
        base_date = task_config["time_reference"]["date"]  # "2025-06-27"
        base_time = time_ref.get("time") or "00:00"
        base_dt = datetime.strptime(f"{base_date} {base_time}", "%Y-%m-%d %H:%M")        
        base_start = base_dt - timedelta(minutes= 5) 

    for cid, link_ids in clusters.items():
        if cluster_id is not None and cid != cluster_id: # íŠ¹ì • í´ëŸ¬ìŠ¤í„° ID only ì²˜ë¦¬
            continue
        cluster_path = os.path.join(output_dir, str(cid))
        os.makedirs(cluster_path, exist_ok=True)
        with Pool(processes=cpu_count() - 2) as pool:
            results = pool.starmap(
                load_tensor_for_link,
                [(lid, base_start, offsets, max_offset, selectedLink, processed_data_dir,'x', Processmode,logger) for lid in link_ids]
            )
        # ìœ íš¨ ê²°ê³¼ í•„í„°ë§
        tensors = []
        valid_link_ids = []
        for tensor, lid,_ in results:
            if tensor is not None:
                tensors.append(tensor)
                valid_link_ids.append(lid)
        
        # ì €ì¥
        if tensors:
            T = min(t.shape[0] for t in tensors)
            tensors = [t[:T] for t in tensors]  # ë™ì¼í•œ Të¡œ ìë¥´ê¸°
            cluster_tensor = np.stack(tensors, axis=2)  # shape: (T, 36, N) â†’ (T, 36, N_links)
            cluster_tensor = cluster_tensor[:, np.newaxis, :, :]  # (T, 1, N_links, 36)
            #cluster_tensor = np.transpose(cluster_tensor, (0, 1, 2, 3))  # ëª…ì‹œì 

            cluster_path = os.path.join(output_dir,str(cid))
            os.makedirs(os.path.dirname(cluster_path), exist_ok=True)
            if mode == 'train':
                mean = np.nanmean(cluster_tensor)  # NaN ê³ ë ¤
                std = np.nanstd(cluster_tensor)
                Scaler = Scaler_tool(mean,std)  # ê³ ì • ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©
                tensors = Scaler.transform(cluster_tensor)
                scaler ={"mean": float(mean), "std": float(std)}
                np.save(os.path.join(cluster_path, f"scaler.npy"),scaler)
                
                for option in ["train", "val", "test"]:
                    if option == "train":
                        indices = np.arange(0, int(T * train_ratio[0]))
                    elif option == "val":
                        indices = np.arange(int(T * train_ratio[0]), int(T * (train_ratio[0] + train_ratio[1])))
                    elif option == "test":
                        indices = np.arange(int(T * (train_ratio[0] + train_ratio[1])), T)
                    split_tensor = tensors[indices]
                    save_path = os.path.join(cluster_path, f"{option}_data.npz")
                    np.savez_compressed(save_path, split_tensor)
#                save_path = os.path.join(cluster_path, f"valid_link_id.npy")
#                np.save(save_path, valid_link_ids)
                logger.info(f"âœ… {option} ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {save_path} | shape={len(valid_link_ids)}")
            elif mode == 'test':  # test ëª¨ë“œ
                scaler=np.load(os.path.join(cluster_path, f"scaler.npy"),allow_pickle=True).item()

                mean = scaler['mean']
                std = scaler['std']
                Scaler = Scaler_tool(mean,std)  # ê³ ì • ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©
                tensors = Scaler.transform(cluster_tensor)
                save_path = os.path.join(cluster_path, f"predict_data.npy")
                np.save(save_path, tensors)
                logger.info(f"âœ… predict_data ì €ì¥ ì™„ë£Œ: {save_path} | shape={tensors.shape}")
        else:
            logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° {cid}ì— ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ")
        if cid ==0 :# timestamp ì €ì¥            
            save_path = os.path.join(processed_data_dir, f"timestamp.npy")
            np.save(save_path, results[0][2])

    return results[0][2] # timestamp

def build_cluster_y_tensor(clusters, processed_data_dir, path_config, task_config, 
                           output_dir, logger,cluster_id=None):
    """
    í´ëŸ¬ìŠ¤í„° ë‹¨ìœ„ë¡œ (T, 1, N_links, 36) í˜•íƒœì˜ í…ì„œë¥¼ ìƒì„±í•˜ì—¬ ì €ì¥

    Args:
        cluster_path (str): LivingLab_clusters.pkl ê²½ë¡œ
        processed_data_dir (str): ë§í¬ë³„ parquet íŒŒì¼ ì €ì¥ í´ë”
        offsets (list[int]): ê´€ì‹¬ ì‹œê°„ ì˜¤í”„ì…‹ (ì˜ˆ: [-60, -55, ..., -5])
        output_dir (str): í…ì„œ ì €ì¥ ê²½ë¡œ
        logger (Logger, optional): ë¡œê¹… ê°ì²´
    """
    os.makedirs(output_dir, exist_ok=True)
    ITS_Link_path = path_config['data']['ITS_info']
    df = DBF(ITS_Link_path, encoding='cp949')
    Linkdata = pd.DataFrame(iter(df))
    selectedLink = Linkdata[['LINK_ID', 'MAX_SPD']]

    timestamp = logger.timestamp  # ë¡œê±°ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ê°€ì ¸ì˜¤ê¸°
    date_only = timestamp[:10]  # '2025-06-27' í˜•ì‹ìœ¼ë¡œ ë‚ ì§œë§Œ ì¶”ì¶œ
    interval=task_config['interval']  # ì‹œê°„ ê°„ê²© (ë¶„ ë‹¨ìœ„)
    interest_y = task_config['interest_y']
    train_ratio = task_config['train_ratio']

    offsets = [interval * i for i in range(interest_y)]

    # ê°€ëŠ¥í•œ base time (ê°€ì¥ ë¹ ë¥¸ offset ê¸°ì¤€ í™•ë³´ëœ ì‹œê°„ ë²”ìœ„)
    min_offset = abs(min(offsets))
    max_offset = interval *interest_y
    start_date = datetime.strptime(task_config['date_range']['start'], "%Y-%m-%d")
    base_start = start_date
    for cid, link_ids in clusters.items():
        if cluster_id is not None and cid != cluster_id: # íŠ¹ì • í´ëŸ¬ìŠ¤í„° ID only ì²˜ë¦¬
            continue
        cluster_path = os.path.join(output_dir, str(cid))
        os.makedirs(cluster_path, exist_ok=True)

        with Pool(processes=cpu_count() // 4) as pool:
            results = pool.starmap(
                load_tensor_for_link,
                [(lid, base_start, offsets, max_offset, selectedLink, processed_data_dir,'y', logger) for lid in link_ids]
            )
        # ìœ íš¨ ê²°ê³¼ í•„í„°ë§
        tensors = []
        valid_link_ids = []
        for tensor, lid,_ in results:
            if tensor is not None:
                tensors.append(tensor)
                valid_link_ids.append(lid)
        
        # ì €ì¥
        if tensors:
            T = min(t.shape[0] for t in tensors)
            tensors = [t[:T] for t in tensors]  # ë™ì¼í•œ Të¡œ ìë¥´ê¸°
            cluster_tensor = np.stack(tensors, axis=2)  # shape: (T, 12, N) â†’ (T, 12, N_links)
            cluster_tensor = cluster_tensor[:, np.newaxis, :, :]  # (T, 1, N_links, 12)
            #cluster_tensor = np.transpose(cluster_tensor, (0, 1, 2, 3))  # ëª…ì‹œì 

            cluster_path = os.path.join(output_dir,str(cid))
            os.makedirs(os.path.dirname(cluster_path), exist_ok=True)
            tensor =cluster_tensor
            
            for option in ["train", "val", "test"]:
                if option == "train":
                    indices = np.arange(0, int(T * train_ratio[0]))
                elif option == "val":
                    indices = np.arange(int(T * train_ratio[0]), int(T * (train_ratio[0] + train_ratio[1])))
                elif option == "test":
                    indices = np.arange(int(T * (train_ratio[0] + train_ratio[1])), T)
                split_tensor = tensor[indices]
                save_path = os.path.join(cluster_path, f"{option}_ydata.npz")
                np.savez_compressed(save_path, split_tensor)
                logger.info(f"âœ… {option} ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {save_path} | shape={split_tensor.shape}")

        else:
            logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° {cid}ì— ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ")
def build_tensor(
    link_ids,
    output_dir_raw,
    path_config,
    task_config,
    output_dir,
    logger,
    mode='test'
):
## for far prediction method

    os.makedirs(output_dir, exist_ok=True)

    # 0) ìƒìˆ˜ë“¤ ê³„ì‚°
    start = datetime.strptime(task_config["date_range"]["start"], "%Y-%m-%d")
    end   = datetime.strptime(task_config["date_range"]["end"],   "%Y-%m-%d")
    base_dates = [start + timedelta(days=i)
                  for i in range((end - start).days + 1)]
    interest = task_config['far_time_process']["interest"]   # e.g. [-21,...,0]
    offsets  = interest[:]
    interval = task_config['interval']  # in minutes, e.g. 5
    train_frac, val_frac, test_frac = task_config['train_ratio']

    L = len(link_ids)
    C = len(offsets) + 1
    T = 1440 // interval
    D = len(base_dates)
    N = D * T

    # 1) HDF5 íŒŒì¼ & ë°ì´í„°ì…‹ ë¯¸ë¦¬ ìƒì„±
    if mode == 'train':
        h5_path = os.path.join(output_dir, "tensors.h5")
    elif mode == 'predict':
        h5_path = os.path.join(output_dir, "predict_tensors.h5")
    s_mode = 'r+' if os.path.exists(h5_path) else 'w'
    hf = h5py.File(h5_path, s_mode)

    if mode == 'train':
        n_train = int(D * train_frac)*T
        n_val   = int(D * val_frac)*T
        n_test  = N- n_train - n_val
        
        if n_train > 0:
            ds_train = hf.create_dataset(
                "train", (n_train, C, L),
                dtype="float16", chunks=(T, C, L), compression="gzip"
            )
        if n_val > 0:
            ds_val   = hf.create_dataset(
                "val",   (n_val,   C, L),
                dtype="float16", chunks=(T, C, L), compression="gzip"
            )
        if n_test > 0:
            ds_test  = hf.create_dataset(
                "test",  (n_test,  C, L),
                dtype="float16", chunks=(T, C, L), compression="gzip"
            )
    else:  # test
        if 'predict' in hf:
            del hf['predict']
        ds_pred = hf.create_dataset(
            "predict", (N, C, L),
            dtype="float16", chunks=(T, C, L), compression="gzip"
        )

    # 2) max_speed dict
    ITS_Link_path = path_config['data']['ITS_info']
    Linkdata = pd.DataFrame(iter(DBF(ITS_Link_path, encoding='cp949')))
    max_speed_dict = dict(zip(Linkdata['LINK_ID'], Linkdata['MAX_SPD']))

    global_idx = 0
    # 3) ë‚ ì§œë³„ë¡œ í•˜ë£¨ì¹˜ë§Œ ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ì„œ ì²˜ë¦¬
    for di, base_date in enumerate(base_dates):
        date_key = base_date.strftime("%Y-%m-%d")
        logger.info(f"[{date_key}] ì²˜ë¦¬ ì‹œì‘ ({di+1}/{D})")

        # a) ì‹œê°„ ì¸ë±ìŠ¤
        times = [
            (base_date + timedelta(minutes=interval*i))
                .time().strftime("%H:%M")
            for i in range(T)
        ]

        # b) speed ì±„ë„ ë°°ì—´ (T, len(offsets), L)
        speeds = np.empty((T, len(offsets), L), dtype=np.float32)
        speeds.fill(np.nan)
        Scaler = Scaler_tool()
        if di == 0:
            np.save(os.path.join(output_dir, "scaler.npy"), Scaler)
        start_idx = global_idx
        end_idx   = start_idx + T
        # ê° offset ì±„ë„ì„ ë°”ë¡œ ì“°ê¸°
        for ci, off in enumerate(offsets):
            d = base_date + timedelta(days=off)
            fn = os.path.join(output_dir_raw, d.strftime("%Y%m%d") + ".parquet")
            if os.path.exists(fn):
                df = pd.read_parquet(fn,
                            columns=["link_id","insert_time","speed"])
                tmp = (
                    df.pivot(index="link_id", columns="insert_time", values="speed")
                    .reindex(index=link_ids, columns=times)
                    .apply(lambda row: row.fillna(max_speed_dict[row.name]), axis=1)
                )
                channel = tmp.values.T.astype(np.float16)  # shape = (T, L)
                del df, tmp
            else:
                channel = np.full((T, L), np.nan, dtype=np.float16)

            # ìŠ¤ì¼€ì¼ë§ë„ ì±„ë„ë³„ë¡œ
            channel = Scaler.transform(channel)  # Scaler_tool ì— ì±„ë„ ë‹¨ìœ„ ë³€í™˜ ë©”ì„œë“œ ê°€ì •

            # HDF5 write (train/val/test í˜¹ì€ predict)


            if mode == 'train':
                # train
                if 'ds_train' in locals():
                    a0, a1 = start_idx, min(end_idx, n_train)
                    if a1 > a0:
                        ds_train[a0:a1, ci, :] = channel[0:(a1-a0)]
                # val
                if 'ds_val' in locals():
                    b0, b1 = max(start_idx, n_train), min(end_idx, n_train+n_val)
                    if b1 > b0:
                        ofs = b0 - start_idx
                        ds_val[b0-n_train:b1-n_train, ci, :] = channel[ofs:ofs+(b1-b0)]
                # test
                if 'ds_test' in locals():
                    c0, c1 = max(start_idx, n_train+n_val), end_idx
                    if c1 > c0:
                        ofs = c0 - start_idx
                        ds_test[c0-(n_train+n_val):c1-(n_train+n_val), ci, :] = channel[ofs:ofs+(c1-c0)]
            else:
                ds_pred[start_idx:end_idx, ci, :] = channel

            del channel
        # c) holiday ì±„ë„ (T, L)
        kr = holidays.KR(years={base_date.year})
        flag = float(base_date in kr)
        hch = np.full((T, L), flag, dtype=np.float32)

        # d) ì±„ë„ ê²°í•© â†’ (T, C, L)

        if mode == 'train':
            # train
            if 'ds_train' in locals():
                a0, a1 = start_idx, min(end_idx, n_train)
                if a1 > a0:
                    ds_train[a0:a1,-1,:] = hch
            # val
            if 'ds_val' in locals():
                b0, b1 = max(start_idx, n_train), min(end_idx, n_train+n_val)
                if b1 > b0:
                    ofs = b0 - start_idx
                    ds_val[b0-n_train:b1-n_train,-1,:] = hch
            # test
            if 'ds_test' in locals():
                c0, c1 = max(start_idx, n_train+n_val), end_idx
                if c1 > c0:
                    ofs = c0 - start_idx
                    ds_test[c0-(n_train+n_val):c1-(n_train+n_val)-1,:] = hch
        else:
            ds_pred[start_idx:end_idx,-1,:] = hch

        global_idx = end_idx
        # ë©”ëª¨ë¦¬ í•´ì œ
        del speeds, hch, 
        gc.collect()
        hf.flush()
    hf.close()
    logger.info(f"âœ… HDF5 ìƒì„± ì™„ë£Œ: {h5_path}")


## for far prediction method
def build_y_tensor(
    link_ids,
    output_dir_raw,
    path_config,
    task_config,
    output_dir,
    logger,
    mode='test'
):
    os.makedirs(output_dir, exist_ok=True)

    # 0) ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
    start = datetime.strptime(task_config["date_range"]["start"], "%Y-%m-%d")
    end   = datetime.strptime(task_config["date_range"]["end"],   "%Y-%m-%d")
    base_dates = [start + timedelta(days=i)
                  for i in range((end - start).days + 1)]
    interval    = task_config['interval']             # in minutes, e.g. 5
    train_frac, val_frac, test_frac = task_config['train_ratio']

    L = len(link_ids)
    T = 1440 // interval     # í•˜ë£¨ ìŠ¤í… ìˆ˜, e.g. 288
    D = len(base_dates)
    N = D * T

    # 1) HDF5 ì—´ê¸° & yìš© ë°ì´í„°ì…‹ ë¯¸ë¦¬ ìƒì„±
    h5_path = os.path.join(output_dir, "y_tensors.h5")
    hf = h5py.File(h5_path, "w")
    Scaler = Scaler_tool()

    if mode == 'train':
        n_train = int(D * train_frac)*T
        n_val   = int(D * val_frac)*T
        n_test  = N - n_train - n_val

        if n_train > 0:
            ds_train = hf.create_dataset(
                "train", (n_train, 1, L),
                dtype="float16", chunks=(T,1,L), compression="gzip"
            )
        if n_val > 0:
            ds_val   = hf.create_dataset(
                "val", (n_val, 1, L),
                dtype="float16", chunks=(T,1,L), compression="gzip"
            )
        if n_test > 0:
            ds_test  = hf.create_dataset(
                "test", (n_test, 1, L),
                dtype="float16", chunks=(T,1,L), compression="gzip"
            )
    else:  # test ëª¨ë“œ
        ds_pred = hf.create_dataset(
            "predict", (N, 1, L),
            dtype="float16", chunks=(T,1,L), compression="gzip"
        )

    # 2) max_speed dict ì¤€ë¹„
    ITS_Link_path = path_config['data']['ITS_info']
    Linkdata = pd.DataFrame(iter(DBF(ITS_Link_path, encoding='cp949')))
    max_speed_dict = dict(zip(Linkdata['LINK_ID'], Linkdata['MAX_SPD']))

    global_idx = 0
    # 3) ë‚ ì§œë³„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    for di, base_date in enumerate(base_dates):
        date_key = base_date.strftime("%Y-%m-%d")
        logger.info(f"[{date_key}] Y tensor ì²˜ë¦¬ ({di+1}/{D})")

        # 3-1) time index
        times = [
            (base_date + timedelta(minutes=interval*i))
                .time().strftime("%H:%M")
            for i in range(T)
        ]

        # 3-2) í•˜ë£¨ì¹˜ speed DataFrame
        fn = os.path.join(output_dir_raw, base_date.strftime("%Y%m%d") + ".parquet")
        if os.path.exists(fn):
            df = pd.read_parquet(fn, columns=["link_id","insert_time","speed"])
        else:
            df = pd.DataFrame(columns=["link_id","insert_time","speed"])
            logger.warning(f"  - {date_key}.parquet ì—†ìŒ â†’ ì „ë¶€ NaN ì²˜ë¦¬")

        # 3-3) pivot + reindex
        tmp = (
            df
            .pivot(index="link_id", columns="insert_time", values="speed")
            .reindex(index=link_ids, columns=times)
        )
        # 3-4) NaN â†’ max_speed
        tmp = tmp.apply(lambda row: row.fillna(max_speed_dict[row.name]), axis=1)

        # 3-5) NumPy `(T, L)` â†’ reshape `(T,1,L)`
        day_y = tmp.values.T.astype(np.float32)
        day_y = day_y.reshape((T,1,L))
        day_y = Scaler.transform(day_y)  # Scaler_tool ì— ì±„ë„ ë‹¨ìœ„ ë³€í™˜ ë©”ì„œë“œ ê°€ì •
        # 4) HDF5ì— ë°”ë¡œ ì“°ê¸°
        start_idx = global_idx
        end_idx   = start_idx + T

        if mode == 'train':
            # train
            if 'ds_train' in locals():
                a0, a1 = start_idx, min(end_idx, n_train)
                if a1 > a0:
                    ds_train[a0:a1] = day_y[0:(a1-a0)]
            # val
            if 'ds_val' in locals():
                b0, b1 = max(start_idx, n_train), min(end_idx, n_train+n_val)
                if b1 > b0:
                    ofs = b0 - start_idx
                    ds_val[b0-n_train:b1-n_train] = day_y[ofs:ofs+(b1-b0)]
            # test
            if 'ds_test' in locals():
                c0, c1 = max(start_idx, n_train+n_val), end_idx
                if c1 > c0:
                    ofs = c0 - start_idx
                    ds_test[c0-(n_train+n_val):c1-(n_train+n_val)] = day_y[ofs:ofs+(c1-c0)]
        else:
            ds_pred[start_idx:end_idx] = day_y

        global_idx = end_idx
        # ë©”ëª¨ë¦¬ í•´ì œ
        del df, tmp, day_y
        gc.collect()
        hf.flush()

    hf.close()
    logger.info(f"âœ… Y HDF5 ìƒì„± ì™„ë£Œ: {h5_path}")


def process_raw_to_cluster_tensor(raw_data_dir, clusters, path_config, task_config, output_dir, logger, mode='train', cluster_id=None, dates=None, exclude_missing_links=False, generate_y=False):
    """
    Raw ë°ì´í„°ì—ì„œ ë°”ë¡œ í´ëŸ¬ìŠ¤í„° í…ì„œë¥¼ ìƒì„±í•˜ëŠ” ìµœì í™”ëœ í•¨ìˆ˜
    - Raw parquet íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ë§í¬ë³„ë¡œ ì •ë¦¬í•˜ê³  í´ëŸ¬ìŠ¤í„° í…ì„œë¡œ ë³€í™˜
    - ì¤‘ê°„ parquet íŒŒì¼ ìƒì„± ì—†ì´ ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ì²˜ë¦¬
    - dates íŒŒë¼ë¯¸í„°ë¡œ íŠ¹ì • ë‚ ì§œë“¤ë§Œ ì²˜ë¦¬
    - generate_y=Trueì¼ ë•Œ y í…ì„œë„ í•¨ê»˜ ìƒì„±
    
    Args:
        exclude_missing_links: Trueë©´ ë¹„ì¡´ì¬í•˜ëŠ” ë§í¬ë¥¼ ì œì™¸, Falseë©´ max_speedë¡œ ì±„ì›€
        generate_y: Trueë©´ y í…ì„œë„ í•¨ê»˜ ìƒì„±
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ì„¤ì • ë¡œë“œ
    interval = task_config['interval']
    interest = task_config['interest']
    interest_y = task_config.get('interest_y', 0)
    train_ratio = task_config['train_ratio']
    
    # x offsets ê³„ì‚°
    offsets = []
    for base in interest:
        offsets.extend([interval * i for i in range(base, base + 12)])
    offsets = sorted(offsets)
    
    # y offsets ê³„ì‚° (generate_yê°€ Trueì¼ ë•Œë§Œ)
    if generate_y:
        y_offsets = [interval * i for i in range(interest_y)]
    
    # base_start ê³„ì‚°
    max_offset = interval * interest_y
    if task_config['mode'] == 'date_range':
        start_date = datetime.strptime(task_config['date_range']['start'], "%Y-%m-%d")
        base_start = start_date
    elif task_config['mode'] == 'time_reference':
        time_ref = task_config["time_reference"]
        base_date = time_ref["date"]
        base_time = time_ref.get("time") or "00:00"
        base_dt = datetime.strptime(f"{base_date} {base_time}", "%Y-%m-%d %H:%M")
        base_start = base_dt - timedelta(minutes=5)
    
    # MAX_SPD ì •ë³´ ë¡œë“œ
    ITS_Link_path = path_config['data']['ITS_info']
    df = DBF(ITS_Link_path, encoding='cp949')
    Linkdata = pd.DataFrame(iter(df))
    selectedLink = Linkdata[['LINK_ID', 'MAX_SPD']]
    max_speed_dict = dict(zip(selectedLink['LINK_ID'], selectedLink['MAX_SPD']))
    
    # exclude_missing_links ì˜µì…˜ ê°€ì ¸ì˜¤ê¸°
    exclude_missing_links = task_config.get('exclude_missing_links', True)
    
    # ì²˜ë¦¬í•  ë‚ ì§œë“¤ ê²°ì •
    if dates is None:
        # datesê°€ ì—†ìœ¼ë©´ ëª¨ë“  parquet íŒŒì¼ ì²˜ë¦¬
        raw_files = sorted([f for f in os.listdir(raw_data_dir) if f.endswith('.parquet')])
        dates_to_process = [f.replace('.parquet', '') for f in raw_files]
    else:
        # datesì—ì„œ ì •ì˜ëœ ë‚ ì§œë“¤ë§Œ ì²˜ë¦¬
        dates_to_process = dates
        raw_files = [f"{date}.parquet" for date in dates]
    
    logger.info(f"ğŸ”¹ Raw íŒŒì¼ {len(raw_files)}ê°œ ì²˜ë¦¬ ì‹œì‘ (ë‚ ì§œ: {dates_to_process[0]} ~ {dates_to_process[-1]})")
    if generate_y:
        logger.info(f"ğŸ”¹ y í…ì„œ ìƒì„± ì˜µì…˜ í™œì„±í™”")
    
    # ì „ì²´ ì‹œê°„ ì¸ë±ìŠ¤ ìƒì„±
    full_time_index = pd.Index(pd.date_range(start="00:00", end="23:59", freq="5min").time)
    
    # ë§í¬ë³„ ë°ì´í„°ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    link_data_dict = {}
    
    # Raw íŒŒì¼ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    for date in dates_to_process:
        # datetime ê°ì²´ë¥¼ YYYYMMDD ë¬¸ìì—´ë¡œ ë³€í™˜
        if isinstance(date, datetime):
            date_str = date.strftime("%Y%m%d")
        else:
            # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            date_str = str(date)
        
        file_path = os.path.join(raw_data_dir, f"{date_str}.parquet")
        
        try:
            # Raw íŒŒì¼ ì½ê¸°
            df = pd.read_parquet(file_path)
            if df.empty:
                logger.warning(f"âš ï¸ ë‚ ì§œ {date}: ì½ì–´ì˜¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            logger.info(f"ğŸ”¹ ë‚ ì§œ {date_str} ì²˜ë¦¬ ì¤‘...")
            
            # ë§í¬ë³„ë¡œ ë°ì´í„° ì •ë¦¬
            df["insert_time"] = pd.to_datetime(df["insert_time"], format="%H:%M").dt.time
            grouped = df.groupby("link_id")
            
            for link_id, group in grouped:
                if link_id not in link_data_dict:
                    link_data_dict[link_id] = []
                
                # ì‹œê°„ë³„ ë°ì´í„° ì •ë¦¬
                group = group[["insert_time", "speed"]]
                group.set_index("insert_time", inplace=True)
                group = group.reindex(full_time_index)
                group["speed"] = group["speed"].astype("float32")
                
                # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                group = group.rename_axis("time").reset_index()
                group["date"] = date_str
                
                link_data_dict[link_id].append(group)
                
        except Exception as e:
            logger.error(f"âŒ ë‚ ì§œ {date_str} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    logger.info(f"âœ… Raw ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(link_data_dict)}ê°œ ë§í¬")
    
    # í´ëŸ¬ìŠ¤í„°ë³„ í…ì„œ ìƒì„±
    for cid, cluster_link_ids in clusters.items():
        if cluster_id is not None and cid != cluster_id:
            continue
            
        cluster_path = os.path.join(output_dir, str(cid))
        os.makedirs(cluster_path, exist_ok=True)
        
        logger.info(f"ğŸ”¹ í´ëŸ¬ìŠ¤í„° {cid} í…ì„œ ìƒì„± ì¤‘... (ë§í¬ ìˆ˜: {len(cluster_link_ids)})")
        
        # í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë§í¬ë“¤ì˜ x í…ì„œ ìƒì„±
        x_tensors = []
        y_tensors = []
        valid_link_ids = []
        
        for lid in cluster_link_ids:
            if lid in link_data_dict:
                try:
                    # ë§í¬ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹˜ê¸°
                    link_df = pd.concat(link_data_dict[lid], ignore_index=True)
                    
                    # x í…ì„œ ìƒì„±
                    x_tensor = create_tensor_from_link_data_fast(
                        link_df, lid, base_start, offsets, max_offset, 
                        max_speed_dict, 'x', task_config['mode'], exclude_missing_links
                    )
                    
                    if x_tensor is not None:
                        x_tensors.append(x_tensor)
                        
                        # y í…ì„œë„ ìƒì„± (generate_yê°€ Trueì¼ ë•Œë§Œ)
                        if generate_y:
                            y_tensor = create_tensor_from_link_data_fast(
                                link_df, lid, base_start, y_offsets, max_offset, 
                                max_speed_dict, 'y', task_config['mode'], exclude_missing_links
                            )
                            if y_tensor is not None:
                                y_tensors.append(y_tensor)
                            else:
                                # y í…ì„œê°€ Noneì´ë©´ x í…ì„œë„ ì œê±°
                                x_tensors.pop()
                                continue
                        
                        valid_link_ids.append(lid)
                        
                except Exception as e:
                    logger.warning(f"ë§í¬ {lid} í…ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        # x í…ì„œ ì €ì¥ ë¡œì§
        if x_tensors:
            T = min(t.shape[0] for t in x_tensors)
            x_tensors = [t[:T] for t in x_tensors]
            cluster_x_tensor = np.stack(x_tensors, axis=2)
            cluster_x_tensor = cluster_x_tensor[:, np.newaxis, :, :]
            
            if mode == 'train':
                mean = np.nanmean(cluster_x_tensor)
                std = np.nanstd(cluster_x_tensor)
                Scaler = Scaler_tool(mean, std)
                x_tensors_scaled = Scaler.transform(cluster_x_tensor)
                scaler = {"mean": float(mean), "std": float(std)}
                np.save(os.path.join(cluster_path, f"scaler.npy"), scaler)
                
                for option in ["train", "val", "test"]:
                    if option == "train":
                        indices = np.arange(0, int(T * train_ratio[0]))
                    elif option == "val":
                        indices = np.arange(int(T * train_ratio[0]), int(T * (train_ratio[0] + train_ratio[1])))
                    elif option == "test":
                        indices = np.arange(int(T * (train_ratio[0] + train_ratio[1])), T)
                    split_tensor = x_tensors_scaled[indices]
                    save_path = os.path.join(cluster_path, f"{option}_data.npz")
                    np.savez_compressed(save_path, split_tensor)
                    
                    # y í…ì„œë„ í•¨ê»˜ ì €ì¥
                    if generate_y and y_tensors:
                        y_tensors_trimmed = [t[:T] for t in y_tensors]
                        cluster_y_tensor = np.stack(y_tensors_trimmed, axis=2)
                        cluster_y_tensor = cluster_y_tensor[:, np.newaxis, :, :]
                        split_y_tensor = cluster_y_tensor[indices]
                        save_y_path = os.path.join(cluster_path, f"{option}_ydata.npz")
                        np.savez_compressed(save_y_path, split_y_tensor)
                        logger.info(f"âœ… {option} x,y ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: x={split_tensor.shape}, y={split_y_tensor.shape}")
                    else:
                        logger.info(f"âœ… {option} x ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: shape={split_tensor.shape}")
                        
            elif mode == 'test':
                scaler = np.load(os.path.join(cluster_path, f"scaler.npy"), allow_pickle=True).item()
                mean = scaler['mean']
                std = scaler['std']
                Scaler = Scaler_tool(mean, std)
                x_tensors_scaled = Scaler.transform(cluster_x_tensor)
                save_path = os.path.join(cluster_path, f"predict_data.npy")
                np.save(save_path, x_tensors_scaled)
                logger.info(f"âœ… predict_data ì €ì¥ ì™„ë£Œ: {save_path} | shape={x_tensors_scaled.shape}")
        else:
            logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° {cid}ì— ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ")
    
    # timestamp ìƒì„± ë° ì €ì¥
    timestamp = logger.timestamp
    save_path = os.path.join(output_dir, f"timestamp.npy")
    np.save(save_path, timestamp)
    return timestamp

def create_tensor_from_link_data(link_df, link_id, base_start, offsets, max_offset, selectedLink, type, Processmode, logger):
    """
    ë§í¬ ë°ì´í„°ì—ì„œ í…ì„œë¥¼ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    """
    # max_speed ì¡°íšŒ
    max_speed = float(
        selectedLink.loc[
            selectedLink['LINK_ID']==link_id,
            'MAX_SPD'
        ].iat[0]
    )
    
    # ì „ì²´ ê°€ëŠ¥í•œ base_time ìƒì„±
    if Processmode == 'date_range':
        end_time = base_start + timedelta(days=1) - timedelta(minutes=max_offset)
    elif Processmode == 'time_reference':
        end_time = base_start + timedelta(minutes=max_offset)
    
    if len(offsets) > 1:
        step = offsets[1] - offsets[0]
    else:
        step = max_offset
    
    all_base_times = pd.date_range(
        start=base_start, end=end_time, freq=f"{step}T"
    )
    
    # ë°ì´í„°í”„ë ˆì„ì„ datetime ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    link_df["datetime"] = pd.to_datetime(
        link_df["date"].astype(str) + " " + link_df["time"].astype(str)
    )
    link_df = link_df.set_index("datetime")["speed"].sort_index()
    
    # ìœ íš¨í•œ base_times í•„í„°
    valid = (all_base_times >= base_start) & (all_base_times <= link_df.index[-1] - timedelta(minutes=max_offset))
    base_times = all_base_times[valid]
    
    link_tensor = []
    for bt in base_times:
        time_points = [bt + timedelta(minutes=o) for o in offsets]
        row = link_df.reindex(time_points).values
        if type == 'x':
            # NaN â†’ max_speed
            row = np.where(np.isnan(row), max_speed, row)
        link_tensor.append(row)
    
    if len(link_tensor) == 0:
        # ìœ íš¨í•œ ì‹œê°„ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´, NaN tensorë¡œ
        nan_tensor = np.full(
            (len(all_base_times), len(offsets)),
            np.nan,
            dtype=float
        )
        return nan_tensor
    
    return np.stack(link_tensor)

def create_tensor_from_link_data_fast(link_df, link_id, base_start, offsets, max_offset, max_speed_dict, type, Processmode, exclude_missing_links=False):
    """
    ìµœì í™”ëœ ë§í¬ ë°ì´í„°ì—ì„œ í…ì„œ ìƒì„±
    
    Args:
        exclude_missing_links: Trueë©´ ë¹„ì¡´ì¬í•˜ëŠ” ë§í¬ë¥¼ ì œì™¸, Falseë©´ max_speedë¡œ ì±„ì›€
    """
    # max_speed ì¡°íšŒ (ë”•ì…”ë„ˆë¦¬ì—ì„œ ì§ì ‘)
    max_speed = max_speed_dict.get(link_id, 60.0)
    
    # ì „ì²´ ê°€ëŠ¥í•œ base_time ìƒì„±
    if Processmode == 'date_range':
        end_time = base_start + timedelta(days=1) - timedelta(minutes=max_offset)
    elif Processmode == 'time_reference':
        end_time = base_start + timedelta(minutes=max_offset)
    
    if len(offsets) > 1:
        step = offsets[1] - offsets[0]
    else:
        step = max_offset
    
    all_base_times = pd.date_range(
        start=base_start, end=end_time, freq=f"{step}T"
    )
    
    # ë°ì´í„°í”„ë ˆì„ì„ datetime ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (ìµœì í™”)
    link_df["datetime"] = pd.to_datetime(
        link_df["date"].astype(str) + " " + link_df["time"].astype(str)
    )
    link_df = link_df.set_index("datetime")["speed"].sort_index()
    
    # ìœ íš¨í•œ base_times í•„í„°
    valid = (all_base_times >= base_start) & (all_base_times <= link_df.index[-1] - timedelta(minutes=max_offset))
    base_times = all_base_times[valid]
    
    # exclude_missing_linksê°€ trueì¼ ë•ŒëŠ” ë°ì´í„°ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜
    if exclude_missing_links and len(base_times) == 0:
        return None
    
    # ë²¡í„°í™”ëœ ì—°ì‚°ìœ¼ë¡œ í…ì„œ ìƒì„±
    link_tensor = []
    for bt in base_times:
        time_points = [bt + timedelta(minutes=o) for o in offsets]
        row = link_df.reindex(time_points).values
        if type == 'x':
            if exclude_missing_links:
                # ê¸°ì¡´ ë°©ì‹: NaN ìœ ì§€
                pass
            else:
                # ìƒˆë¡œìš´ ë°©ì‹: NaN â†’ max_speed
                row = np.where(np.isnan(row), max_speed, row)
        link_tensor.append(row)
    
    if len(link_tensor) == 0:
        # ìœ íš¨í•œ ì‹œê°„ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´, exclude_missing_linksì— ë”°ë¼ ì²˜ë¦¬
        if exclude_missing_links:
            return None  # ê¸°ì¡´ ë°©ì‹: None ë°˜í™˜
        else:
            # ìƒˆë¡œìš´ ë°©ì‹: NaN tensor ë°˜í™˜
            nan_tensor = np.full(
                (len(all_base_times), len(offsets)),
                np.nan,
                dtype=np.float32
            )
            return nan_tensor
    
    # np.stackì„ ë°–ì—ì„œ ì²˜ë¦¬
    try:
        tensor = np.stack(link_tensor)
        
        # exclude_missing_linksê°€ trueì¼ ë•ŒëŠ” NaN ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ None ë°˜í™˜
        if exclude_missing_links:
            nan_ratio = np.isnan(tensor).sum() / tensor.size
            if nan_ratio > 0.8:  # 80% ì´ìƒì´ NaNì´ë©´ ì œì™¸
                return None
        
        return tensor
    except Exception as e:
        # np.stack ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
        return None

def build_cluster_y_tensor_optimized(raw_data_dir, clusters, path_config, task_config, output_dir, logger, cluster_id=None, dates=None):
    """
    Raw ë°ì´í„°ì—ì„œ ì§ì ‘ í´ëŸ¬ìŠ¤í„° y í…ì„œ ìƒì„± (ìµœì í™”ëœ ë°©ì‹)
    - Raw parquet íŒŒì¼ë“¤ì„ ì½ì–´ì„œ í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì§ì ‘ y í…ì„œ ìƒì„±
    - ì¤‘ê°„ parquet íŒŒì¼ ìƒì„± ì—†ì´ ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ì²˜ë¦¬
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ì„¤ì • ë¡œë“œ
    interval = task_config['interval']
    interest_y = task_config['interest_y']
    train_ratio = task_config['train_ratio']
    
    # y offsets ê³„ì‚° (ì˜ˆì¸¡í•  ë¯¸ë˜ ì‹œê°„)
    offsets = [interval * i for i in range(interest_y)]
    
    # base_start ê³„ì‚°
    start_date = datetime.strptime(task_config['date_range']['start'], "%Y-%m-%d")
    base_start = start_date
    
    # MAX_SPD ì •ë³´ ë¡œë“œ
    ITS_Link_path = path_config['data']['ITS_info']
    df = DBF(ITS_Link_path, encoding='cp949')
    Linkdata = pd.DataFrame(iter(df))
    selectedLink = Linkdata[['LINK_ID', 'MAX_SPD']]
    max_speed_dict = dict(zip(selectedLink['LINK_ID'], selectedLink['MAX_SPD']))
    
    # ì²˜ë¦¬í•  ë‚ ì§œë“¤ ê²°ì •
    if dates is None:
        raw_files = sorted([f for f in os.listdir(raw_data_dir) if f.endswith('.parquet')])
        dates_to_process = [f.replace('.parquet', '') for f in raw_files]
    else:
        dates_to_process = dates
    
    logger.info(f"ğŸ”¹ Raw íŒŒì¼ {len(dates_to_process)}ê°œì—ì„œ y í…ì„œ ìƒì„± ì‹œì‘")
    
    # ì „ì²´ ì‹œê°„ ì¸ë±ìŠ¤ ìƒì„±
    full_time_index = pd.Index(pd.date_range(start="00:00", end="23:59", freq="5min").time)
    
    # ë§í¬ë³„ ë°ì´í„°ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    link_data_dict = {}
    
    # Raw íŒŒì¼ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    for date in dates_to_process:
        file_path = os.path.join(raw_data_dir, f"{date}.parquet")
        
        try:
            # Raw íŒŒì¼ ì½ê¸°
            df = pd.read_parquet(file_path)
            if df.empty:
                logger.warning(f"âš ï¸ ë‚ ì§œ {date}: ì½ì–´ì˜¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            logger.info(f"ğŸ”¹ ë‚ ì§œ {date} y í…ì„œìš© ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
            
            # ë§í¬ë³„ë¡œ ë°ì´í„° ì •ë¦¬
            df["insert_time"] = pd.to_datetime(df["insert_time"], format="%H:%M").dt.time
            grouped = df.groupby("link_id")
            
            for link_id, group in grouped:
                if link_id not in link_data_dict:
                    link_data_dict[link_id] = []
                
                # ì‹œê°„ë³„ ë°ì´í„° ì •ë¦¬
                group = group[["insert_time", "speed"]]
                group.set_index("insert_time", inplace=True)
                group = group.reindex(full_time_index)
                group["speed"] = group["speed"].astype("float32")
                
                # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                group = group.rename_axis("time").reset_index()
                group["date"] = date
                
                link_data_dict[link_id].append(group)
                
        except Exception as e:
            logger.error(f"âŒ ë‚ ì§œ {date} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    logger.info(f"âœ… Raw ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(link_data_dict)}ê°œ ë§í¬")
    
    # í´ëŸ¬ìŠ¤í„°ë³„ y í…ì„œ ìƒì„±
    for cid, cluster_link_ids in clusters.items():
        if cluster_id is not None and cid != cluster_id:
            continue
            
        cluster_path = os.path.join(output_dir, str(cid))
        os.makedirs(cluster_path, exist_ok=True)
        
        logger.info(f"ğŸ”¹ í´ëŸ¬ìŠ¤í„° {cid} y í…ì„œ ìƒì„± ì¤‘... (ë§í¬ ìˆ˜: {len(cluster_link_ids)})")
        
        # í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë§í¬ë“¤ì˜ y í…ì„œ ìƒì„±
        tensors = []
        valid_link_ids = []
        
        for lid in cluster_link_ids:
            if lid in link_data_dict:
                try:
                    # ë§í¬ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹˜ê¸°
                    link_df = pd.concat(link_data_dict[lid], ignore_index=True)
                    
                    # y í…ì„œ ìƒì„± (ë¯¸ë˜ ì˜ˆì¸¡ìš©)
                    tensor = create_tensor_from_link_data_fast(
                        link_df, lid, base_start, offsets, max_offset, 
                        max_speed_dict, 'y', 'date_range'
                    )
                    
                    if tensor is not None:
                        tensors.append(tensor)
                        valid_link_ids.append(lid)
                        
                except Exception as e:
                    logger.warning(f"ë§í¬ {lid} y í…ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        # y í…ì„œ ì €ì¥ ë¡œì§
        if tensors:
            T = min(t.shape[0] for t in tensors)
            tensors = [t[:T] for t in tensors]
            cluster_tensor = np.stack(tensors, axis=2)
            cluster_tensor = cluster_tensor[:, np.newaxis, :, :]  # (T, 1, N_links, interest_y)
            
            # train/val/test ë¶„í•  ì €ì¥
            for option in ["train", "val", "test"]:
                if option == "train":
                    indices = np.arange(0, int(T * train_ratio[0]))
                elif option == "val":
                    indices = np.arange(int(T * train_ratio[0]), int(T * (train_ratio[0] + train_ratio[1])))
                elif option == "test":
                    indices = np.arange(int(T * (train_ratio[0] + train_ratio[1])), T)
                split_tensor = cluster_tensor[indices]
                save_path = os.path.join(cluster_path, f"{option}_ydata.npz")
                np.savez_compressed(save_path, split_tensor)
                logger.info(f"âœ… {option} y ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {save_path} | shape={split_tensor.shape}")
        else:
            logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° {cid}ì— ìœ íš¨í•œ y ë°ì´í„° ì—†ìŒ")
    
    logger.info(f"âœ… í´ëŸ¬ìŠ¤í„° y í…ì„œ ìƒì„± ì™„ë£Œ: {output_dir}ì— ì €ì¥ë¨")


# ëª¨ë“ˆ ë ˆë²¨ í•¨ìˆ˜ë“¤ (multiprocessingì„ ìœ„í•´)
def process_cluster_batch_mp(args):
    """multiprocessingì„ ìœ„í•œ í´ëŸ¬ìŠ¤í„° ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜"""
    cid, link_ids, timestamps_with_offsets, date_data_cache, max_speed_dict, x_offsets, y_offsets, generate_y = args
    
    x_batch_data = []
    y_batch_data = []
    
    for timestamp_data in timestamps_with_offsets:
        current_timestamp, x_offset_data, y_offset_data = timestamp_data
        
        # x ë°ì´í„° ì²˜ë¦¬
        x_row = process_links_vectorized_mp(link_ids, current_timestamp, x_offsets, date_data_cache, max_speed_dict)
        x_batch_data.append(x_row)
        
        # y ë°ì´í„° ì²˜ë¦¬
        if generate_y and y_offsets:
            y_row = process_links_vectorized_mp(link_ids, current_timestamp, y_offsets, date_data_cache, max_speed_dict)
            y_batch_data.append(y_row)
    
    return cid, x_batch_data, y_batch_data

def process_links_vectorized_mp(link_ids, current_timestamp, offsets, date_data_cache, max_speed_dict):
    """multiprocessing í˜¸í™˜ vectorized ë§í¬ ì²˜ë¦¬"""
    n_links = len(link_ids)
    n_features = len(offsets)
    result = np.zeros((n_links, n_features), dtype=np.float32)
    
    # ì˜¤í”„ì…‹ë³„ë¡œ í•„ìš”í•œ ë‚ ì§œì™€ ì‹œê°„ ë¯¸ë¦¬ ê³„ì‚°
    offset_data = {}
    for feat_idx, offset in enumerate(offsets):
        target_dt = current_timestamp + timedelta(minutes=offset)
        target_date_str = target_dt.strftime("%Y%m%d")
        target_time = target_dt.time()
        
        if target_date_str not in offset_data:
            offset_data[target_date_str] = {}
        if target_time not in offset_data[target_date_str]:
            offset_data[target_date_str][target_time] = []
        offset_data[target_date_str][target_time].append(feat_idx)
    
    # ë‚ ì§œë³„, ì‹œê°„ë³„ë¡œ ì¼ê´„ ì¡°íšŒ
    for date_str, time_dict in offset_data.items():
        if date_str in date_data_cache and date_data_cache[date_str] is not None:
            raw_data = date_data_cache[date_str]
            
            for target_time, feat_indices in time_dict.items():
                # í•´ë‹¹ ì‹œê°„ì˜ ëª¨ë“  ë§í¬ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ
                time_data = raw_data[raw_data['insert_time'] == target_time]
                
                if not time_data.empty:
                    # pandasì˜ mergeë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥¸ ì¡°íšŒ
                    link_df = pd.DataFrame({'link_id': link_ids})
                    merged = link_df.merge(time_data[['link_id', 'speed']], on='link_id', how='left')
                    
                    for link_idx, (link_id, speed) in enumerate(zip(merged['link_id'], merged['speed'])):
                        speed_value = speed if not pd.isna(speed) else max_speed_dict.get(link_id, 60.0)
                        for feat_idx in feat_indices:
                            result[link_idx, feat_idx] = speed_value
                else:
                    # ì‹œê°„ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ë§í¬ì— ëŒ€í•´ max_speed ì‚¬ìš©
                    for link_idx, link_id in enumerate(link_ids):
                        speed_value = max_speed_dict.get(link_id, 60.0)
                        for feat_idx in feat_indices:
                            result[link_idx, feat_idx] = speed_value
        else:
            # ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ë§í¬ì— ëŒ€í•´ max_speed ì‚¬ìš©
            for target_time, feat_indices in time_dict.items():
                for link_idx, link_id in enumerate(link_ids):
                    speed_value = max_speed_dict.get(link_id, 60.0)
                    for feat_idx in feat_indices:
                        result[link_idx, feat_idx] = speed_value
    
    return result

def process_raw_to_cluster_tensor_h5(raw_data_dir, clusters, path_config, task_config, output_dir, logger, mode='train', cluster_id=None, dates=None, generate_y=False):
    """
    ìµœê³ ì„±ëŠ¥ H5 ê¸°ë°˜ í…ì„œ ìƒì„± - multiprocessing + vectorized
    - multiprocessingìœ¼ë¡œ ì§„ì§œ ë³‘ë ¬ ì²˜ë¦¬ (GIL ìš°íšŒ)
    - vectorized pandas ì—°ì‚°ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
    - ëª¨ë“  í´ëŸ¬ìŠ¤í„° H5 íŒŒì¼ì„ ë™ì‹œì— ì—´ì–´ì„œ ì²˜ë¦¬
    """
    import h5py
    import multiprocessing as mp
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ì„¤ì • ë¡œë“œ
    interval = task_config['interval']
    interest = task_config['interest']
    interest_y = task_config.get('interest_y', 0)
    train_ratio = task_config['train_ratio']
    
    # x offsets ê³„ì‚°
    x_offsets = []
    for base in interest:
        x_offsets.extend([interval * i for i in range(base, base + 12)])
    x_offsets = sorted(x_offsets)
    
    # y offsets ê³„ì‚°
    y_offsets = []
    if generate_y and interest_y > 0:
        y_offsets = [interval * i for i in range(interest_y)]
    elif generate_y:
        y_offsets = [interval * i for i in range(12)]  # ê¸°ë³¸ê°’
    
    # ê¸°ê°„ ì„¤ì •
    start_date = datetime.strptime(task_config['date_range']['start'], "%Y-%m-%d")
    end_date = datetime.strptime(task_config['date_range']['end'], "%Y-%m-%d")
    
    # ì „ì²´ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    total_minutes = int((end_date - start_date).total_seconds() / 60)
    total_timestamps = total_minutes // interval
    
    # train/val/test ë¶„í• 
    train_end = int(total_timestamps * train_ratio[0])
    val_end = int(total_timestamps * (train_ratio[0] + train_ratio[1]))
    
    split_info = {
        'train': (0, train_end),
        'val': (train_end, val_end),
        'test': (val_end, total_timestamps)
    }
    
    logger.info(f"ğŸ¯ ì „ì²´ íƒ€ì„ìŠ¤íƒ¬í”„: {total_timestamps}ê°œ")
    logger.info(f"ğŸ“Š ë°ì´í„° ë¶„í• : train={train_end}, val={val_end-train_end}, test={total_timestamps-val_end}")
    
    # MAX_SPD ì •ë³´ ë¡œë“œ
    ITS_Link_path = path_config['data']['ITS_info']
    df = DBF(ITS_Link_path, encoding='cp949')
    Linkdata = pd.DataFrame(iter(df))
    selectedLink = Linkdata[['LINK_ID', 'MAX_SPD']]
    max_speed_dict = dict(zip(selectedLink['LINK_ID'], selectedLink['MAX_SPD']))
    
    # í´ëŸ¬ìŠ¤í„° í•„í„°ë§
    if cluster_id is not None:
        filtered_clusters = {cluster_id: clusters[cluster_id]}
    else:
        filtered_clusters = clusters  # ëª¨ë“  í´ëŸ¬ìŠ¤í„° ì²˜ë¦¬
    
    logger.info(f"ğŸ”¹ {len(filtered_clusters)}ê°œ í´ëŸ¬ìŠ¤í„° ìµœê³ ì„±ëŠ¥ H5 ì²˜ë¦¬ ì¤‘...")
    
    # ê° í´ëŸ¬ìŠ¤í„°ë³„ ì •ë³´ ë° H5 íŒŒì¼ ê²½ë¡œ ì¤€ë¹„
    cluster_info = {}
    for cid, cluster_link_ids in filtered_clusters.items():
        cluster_path = os.path.join(output_dir, str(cid))
        os.makedirs(cluster_path, exist_ok=True)
        
        N_links = len(cluster_link_ids)
        x_features = len(x_offsets)
        y_features = len(y_offsets) if generate_y and y_offsets else 0
        
        cluster_info[cid] = {
            'link_ids': cluster_link_ids,
            'N_links': N_links,
            'x_features': x_features,
            'y_features': y_features,
            'cluster_path': cluster_path
        }
        
        logger.info(f"  ğŸ“‹ í´ëŸ¬ìŠ¤í„° {cid}: {N_links}ê°œ ë§í¬")
    
    # ê° splitë³„ë¡œ ëª¨ë“  í´ëŸ¬ìŠ¤í„° H5 íŒŒì¼ ë™ì‹œ ì²˜ë¦¬
    for split_name, (start_idx, end_idx) in split_info.items():
        if start_idx >= end_idx:
            continue
            
        T = end_idx - start_idx
        logger.info(f"ğŸ”¹ {split_name} ì„¸íŠ¸ ì²˜ë¦¬: {T}ê°œ íƒ€ì„ìŠ¤íƒ¬í”„")
        
        # ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì˜ H5 íŒŒì¼ë“¤ ë™ì‹œì— ì—´ê¸°
        h5_files = {}
        x_datasets = {}
        y_datasets = {}
        
        for cid, info in cluster_info.items():
            # H5 íŒŒì¼ ê²½ë¡œ
            if mode == 'train':
                h5_path = os.path.join(info['cluster_path'], f"{split_name}_data.h5")
            else:
                h5_path = os.path.join(info['cluster_path'], f"predict_data.h5")
            
            # H5 íŒŒì¼ ì—´ê¸°
            h5_files[cid] = h5py.File(h5_path, 'w')
            
            # ë°ì´í„°ì…‹ ìƒì„±
            x_datasets[cid] = h5_files[cid].create_dataset('x_data', (T, 1, info['N_links'], info['x_features']), dtype=np.float32)
            
            if generate_y and info['y_features'] > 0:
                y_datasets[cid] = h5_files[cid].create_dataset('y_data', (T, 1, info['N_links'], info['y_features']), dtype=np.float32)
            
            logger.info(f"    ğŸ“ í´ëŸ¬ìŠ¤í„° {cid} H5: {h5_path}")
            logger.info(f"      ğŸ“ x: {x_datasets[cid].shape}")
            if cid in y_datasets:
                logger.info(f"      ğŸ“ y: {y_datasets[cid].shape}")
        
        # ë°°ì¹˜ë³„ ì²˜ë¦¬ (í•˜ë£¨ì”©)
        batch_size = 288  # í•˜ë£¨ì¹˜ íƒ€ì„ìŠ¤íƒ¬í”„
        n_batches = (T + batch_size - 1) // batch_size
        
        for batch_idx in range(n_batches):
            start_t = batch_idx * batch_size
            end_t = min(start_t + batch_size, T)
            
            logger.info(f"  ğŸ”„ ë°°ì¹˜ {batch_idx+1}/{n_batches}: {start_t}~{end_t-1} íƒ€ì„ìŠ¤íƒ¬í”„")
            
            # í•´ë‹¹ ë°°ì¹˜ì—ì„œ í•„ìš”í•œ ë‚ ì§œë“¤ ë¯¸ë¦¬ í™•ì¸
            batch_dates = set()
            for t_idx in range(start_t, end_t):
                global_t_idx = start_idx + t_idx
                current_timestamp = start_date + timedelta(minutes=global_t_idx * interval)
                batch_dates.add(current_timestamp.strftime("%Y%m%d"))
            
            # í•„ìš”í•œ ë‚ ì§œë³„ ë°ì´í„° ë¯¸ë¦¬ ë¡œë“œ (í•œ ë²ˆë§Œ!)
            date_data_cache = {}
            for date_str in batch_dates:
                file_path = os.path.join(raw_data_dir, f"{date_str}.parquet")
                try:
                    raw_data = pd.read_parquet(file_path)
                    if not raw_data.empty:
                        raw_data["insert_time"] = pd.to_datetime(raw_data["insert_time"], format="%H:%M").dt.time
                        raw_data = raw_data[["link_id", "insert_time", "speed"]]
                        raw_data["speed"] = raw_data["speed"].astype("float32")
                        date_data_cache[date_str] = raw_data
                        logger.info(f"    ğŸ“… {date_str} ë¡œë“œ ì™„ë£Œ")
                    else:
                        date_data_cache[date_str] = None
                except Exception as e:
                    logger.warning(f"    âš ï¸ {date_str} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    date_data_cache[date_str] = None
            
            # ë°°ì¹˜ ë‚´ íƒ€ì„ìŠ¤íƒ¬í”„ë³„ ì²˜ë¦¬ (ë‹¨ìˆœ ìˆœì°¨ ë°©ì‹)
            logger.info(f"    ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘...")
            
            for t_idx in range(start_t, end_t):
                global_t_idx = start_idx + t_idx
                current_timestamp = start_date + timedelta(minutes=global_t_idx * interval)
                
                # ëª¨ë“  í´ëŸ¬ìŠ¤í„° ë™ì‹œ ì²˜ë¦¬ (vectorized)
                for cid, info in cluster_info.items():
                    # x ë°ì´í„°ë¥¼ vectorizedë¡œ ì²˜ë¦¬
                    x_row = process_links_vectorized_mp(info['link_ids'], current_timestamp, x_offsets, date_data_cache, max_speed_dict)
                    x_datasets[cid][t_idx, 0, :, :] = x_row
                    
                    # y ë°ì´í„°ë¥¼ vectorizedë¡œ ì²˜ë¦¬
                    if generate_y and info['y_features'] > 0:
                        y_row = process_links_vectorized_mp(info['link_ids'], current_timestamp, y_offsets, date_data_cache, max_speed_dict)
                        y_datasets[cid][t_idx, 0, :, :] = y_row
                
                # ì§„í–‰ ìƒí™© ë¡œê¹… (10%ë§ˆë‹¤)
                if (t_idx - start_t + 1) % max(1, (end_t - start_t) // 10) == 0:
                    progress = (t_idx - start_t + 1) / (end_t - start_t) * 100
                    logger.info(f"      ğŸ“ˆ ì§„í–‰: {progress:.1f}% ({t_idx - start_t + 1}/{end_t - start_t})")
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            progress = (batch_idx + 1) / n_batches * 100
            logger.info(f"    âœ… ìˆœì°¨ ë°°ì¹˜ {batch_idx+1} ì™„ë£Œ ({progress:.1f}%), ë©”ëª¨ë¦¬ ì •ë¦¬ë¨")
            
            # ë°°ì¹˜ ì™„ë£Œ í›„ ìºì‹œ ì •ë¦¬
            del date_data_cache
        
        # ëª¨ë“  H5 íŒŒì¼ ë‹«ê¸°
        for cid in h5_files:
            h5_files[cid].close()
            logger.info(f"  âœ… í´ëŸ¬ìŠ¤í„° {cid} {split_name} H5 íŒŒì¼ ì™„ë£Œ")
    
    # timestamp ìƒì„± ë° ì €ì¥
    timestamp = logger.timestamp
    np.save(os.path.join(output_dir, "timestamp.npy"), timestamp)
    
    logger.info(f"ğŸ‰ ìµœê³ ì„±ëŠ¥ multiprocessing H5 ê¸°ë°˜ í…ì„œ ìƒì„± ì™„ë£Œ!")
    return timestamp


def get_speed_value_batch(target_dt, link_id, date_data_cache, max_speed_dict):
    """ë°°ì¹˜ ì²˜ë¦¬ìš© ì†ë„ ê°’ ì¡°íšŒ"""
    target_date_str = target_dt.strftime("%Y%m%d")
    target_time = target_dt.time()
    
    # ìºì‹œëœ ë°ì´í„°ì—ì„œ ì¡°íšŒ
    if target_date_str in date_data_cache and date_data_cache[target_date_str] is not None:
        raw_data = date_data_cache[target_date_str]
        link_data = raw_data[(raw_data['link_id'] == link_id) & (raw_data['insert_time'] == target_time)]
        
        if len(link_data) > 0:
            speed = link_data['speed'].iloc[0]
            return speed if not pd.isna(speed) else max_speed_dict.get(link_id, 60.0)
    
    # ê¸°ë³¸ê°’ ë°˜í™˜
    return max_speed_dict.get(link_id, 60.0)

