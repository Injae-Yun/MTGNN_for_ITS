import os
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from dbfread import DBF
from collections import defaultdict
import h5py
import gc
from Utils.share import Scaler_tool
import holidays
__all__ = [
    "process_raw_to_cluster_tensor_pivot",
    "process_predict_to_cluster_tensor_pivot",
]

def _build_pivot_for_date(
    date_str: str,
    raw_data_dir: str,
    all_links: np.ndarray,
    link_to_idx: dict[int,int],
    max_speed_dict: dict[int,float]
) -> np.ndarray:
    """ì£¼ì–´ì§„ ë‚ ì§œ(YYYYMMDD)ì˜ raw parquet â†’ (links, 288) pivot ë°°ì—´ ë°˜í™˜"""

    # 1) ëª¨ë“  ë§í¬ë³„ ê¸°ë³¸ê°’ ë°°ì—´(288ì¹¸) ìƒì„± (max_speed_dict ê¸°ë°˜)
    default_speeds = np.array(
        [max_speed_dict.get(lid, 60.0) for lid in all_links],
        dtype=np.float32
    )
    # ê° ë§í¬ë§ˆë‹¤ 288ê°œ ìŠ¬ë¡¯ìœ¼ë¡œ ë³µì œ
    pivot = default_speeds[:, None].repeat(288, axis=1)

    # 2) íŒŒì¼ ì—†ìŒ í˜¹ì€ ë¹ˆ íŒŒì¼ì¼ ë•Œ ê¸°ë³¸ ë°˜í™˜
    file_path = os.path.join(raw_data_dir, f"{date_str}.parquet")
    if not os.path.exists(file_path):
        return pivot

    df = pd.read_parquet(file_path)
    df["link_id"] = df["link_id"].astype(int)
    if df.empty:
        return pivot

    # 3) slot ê³„ì‚° (0~287)
    t = pd.to_datetime(df["insert_time"], format="%H:%M")
    slots = (t.dt.hour * 12 + t.dt.minute // 5)
    mask_bad = slots.isna()
    # 4) link_id â†’ pivot row index ë§¤í•‘, ë§¤í•‘ ì‹¤íŒ¨í•œ í–‰ì€ í•„í„°ë§
    mapped = df["link_id"].map(link_to_idx)
    valid = mapped.notna()
    mask = (~mask_bad) & valid
    rows   = mapped[mask].astype(np.int32).to_numpy()
    slots  = slots[mask]
    slots = slots.to_numpy(dtype=np.int32)
    speeds = df.loc[mask, "speed"].to_numpy(dtype=np.float32)
     
    del df, t, mapped, valid, mask  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì‚­ì œ
    # 5) ë²¡í„° ì¸ë±ì‹± í•œ ë²ˆì— ê°’ ë®ì–´ì“°ê¸°
    pivot[rows, slots] = speeds

    return pivot

def for_LGBM_build_tensors(
    raw_data_dir: str,
    link_ids: list[int],
    path_config: dict,
    task_config: dict,
    output_dir: str,
    mode = 'train',
    logger=None
    ):
    """X(tensors.h5)ì™€ Y(y_tensors.h5)ë¥¼ í†µí•©ëœ ë¡œì§ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    os.makedirs(output_dir, exist_ok=True)

    # --- 1) Task ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸° ---
    logger.info(f"1. ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸° (mode: {mode})")
    start_date = datetime.strptime(task_config["date_range"]["start"], "%Y-%m-%d")
    end_date = datetime.strptime(task_config["date_range"]["end"], "%Y-%m-%d")
    interval = task_config["interval"]  # e.g., 5

    # X í”¼ì²˜ ì„¤ì • (build_tensor ë¡œì§)
    x_offsets = task_config['far_time_process']["interest"]  # e.g., [-21, -14, -7, -1, 0]
    
    # Y í”¼ì²˜ ì„¤ì • (build_y_tensor ë¡œì§)
    # yëŠ” ì˜ˆì¸¡ ì‹œì (base_dt)ì˜ ê°’ì´ë¯€ë¡œ offsetì´ 0ì¸ ê²ƒê³¼ ê°™ìŒ

    all_links = np.array(link_ids, dtype=int)
    link_to_idx = {lid: i for i, lid in enumerate(all_links)}
    
    ITS_path = path_config["data"]["ITS_info"]
    link_df = pd.DataFrame(iter(DBF(ITS_path, encoding="cp949")))
    link_df["LINK_ID"] = link_df["LINK_ID"].astype(int)
    max_speed_dict = dict(zip(link_df["LINK_ID"], link_df["MAX_SPD"]))
    kr_holidays = holidays.KR()

    # --- 3) ë°ì´í„°ì…‹ í¬ê¸° ë° ë¶„í•  ê³„íš ---
    logger.info("2. ë°ì´í„°ì…‹ í¬ê¸° ë° íŒŒì¼ ê²½ë¡œ ì„¤ì •")
    total_days = (end_date - start_date).days + 1
    T = 1440 // interval  # í•˜ë£¨ ë‹¹ íƒ€ì„ìŠ¤íƒ¬í”„ ìˆ˜
    L = len(all_links)    # ë§í¬ ìˆ˜

    if mode == 'train':
        C = len(x_offsets) + 1 # X ì±„ë„ ìˆ˜ (speed ì±„ë„ + holiday ì±„ë„)
    elif mode == 'predict':
        C = len(x_offsets)  # ì˜ˆì¸¡ ì‹œì—ëŠ” holiday ì±„ë„ì´ í•„ìš” ì—†ìŒ

    if mode == 'train':
        N = total_days * T # ì „ì²´ ìœ íš¨ ìƒ˜í”Œ ìˆ˜
        train_frac, val_frac, _ = task_config['train_ratio']
        n_train = int(total_days * train_frac) * T
        n_val = int(total_days * val_frac) * T
        n_test = N - n_train - n_val

        splits = {"train": (0, n_train), "val": (n_train, n_train + n_val), "test": (n_train + n_val, N)}
        logger.info(f"í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±. ì „ì²´ ìƒ˜í”Œ ìˆ˜: {N} (Train: {n_train}, Val: {n_val}, Test: {n_test})")
        h5_x_path = os.path.join(output_dir, "tensors.h5")
        h5_y_path = os.path.join(output_dir, "y_tensors.h5")
    elif mode == 'predict':
        N = total_days * T
        logger.info(f"ì¶”ë¡  ë°ì´í„°ì…‹ ìƒì„±. ì „ì²´ ìƒ˜í”Œ ìˆ˜: {N}")
        h5_x_path = os.path.join(output_dir, "predict_tensors.h5")
        h5_y_path = None  # ì¶”ë¡  ì‹œì—ëŠ” Y í…ì„œ ë¶ˆí•„ìš”
    else:
        raise ValueError("modeëŠ” 'train' ë˜ëŠ” 'predict' ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    hf_y = h5py.File(h5_y_path, "w") if h5_y_path else None
    with h5py.File(h5_x_path, "w") as hf_x:
        # ë°ì´í„°ì…‹ ìƒì„±
        if mode == 'train':
            ds_x = {s: hf_x.create_dataset(s, (size, C, L), dtype="float16") for s, (_, size) in {"train": (0, n_train), "val": (0, n_val), "test": (0, n_test)}.items() if size > 0}
            ds_y = {s: hf_y.create_dataset(s, (size, 1, L), dtype="float16") for s, (_, size) in {"train": (0, n_train), "val": (0, n_val), "test": (0, n_test)}.items() if size > 0}
        else: # predict ëª¨ë“œ
            ds_x = {"predict": hf_x.create_dataset("predict", (N, C, L), dtype="float16")}
            ds_y = {} # Y ë°ì´í„°ì…‹ ì—†ìŒ
        # --- 5) ë‚ ì§œë³„ ì‘ì—… ê³„íš ìˆ˜ë¦½ (í•µì‹¬) ---
        logger.info("3. ì‘ì—… ê³„íš ìˆ˜ë¦½")
        # key: 'YYYYMMDD', value: list of (target_split, target_idx, base_dt)
        date_tasks = defaultdict(list)
        if mode == 'train':
            for i in range(total_days):
                base_date = start_date + timedelta(days=i)
                for t_idx in range(T):
                    current_sample_idx = i * T + t_idx
                    current_split, split_rel_idx = "", 0
                    if current_sample_idx < splits["train"][1]:
                        current_split, split_rel_idx = "train", current_sample_idx
                    elif current_sample_idx < splits["val"][1]:
                        current_split, split_rel_idx = "val", current_sample_idx - splits["val"][0]
                    else:
                        current_split, split_rel_idx = "test", current_sample_idx - splits["test"][0]
                    
                    date_tasks[base_date.strftime("%Y%m%d")].append(("Y", current_split, split_rel_idx, t_idx))
                    for chan_idx, offset in enumerate(x_offsets):
                        fetch_date = base_date + timedelta(days=offset)
                        date_tasks[fetch_date.strftime("%Y%m%d")].append(("X", current_split, split_rel_idx, t_idx, chan_idx))
        else: # predict
            for i in range(total_days):
                base_date = start_date + timedelta(days=i)
                for t_idx in range(T):
                    current_sample_idx = i * T + t_idx
                    for chan_idx, offset in enumerate(x_offsets):
                        fetch_date = base_date + timedelta(days=offset)
                        date_tasks[fetch_date.strftime("%Y%m%d")].append(("X", "predict", current_sample_idx, t_idx, chan_idx))

        # --- 4) ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ ---
        logger.info("4. ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ ì‹œì‘")
        Scaler = Scaler_tool()
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ pivot ë¡œë“œ
        for d_str, tasks in sorted(date_tasks.items()):
            logger.info(f"  - Pivot ë¡œë”©: {d_str}")
            pivot = _build_pivot_for_date(d_str, raw_data_dir, all_links, link_to_idx, max_speed_dict)
            
            # Yì™€ X ëª¨ë‘ ë™ì¼í•œ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ë³€í™˜
            pivot_scaled = Scaler.transform(pivot).astype(np.float16)
            
            for task_type, split, s_idx, t_idx, *chan_info in tasks:
                if task_type == "Y":
                    ds_y[split][s_idx, 0, :] = pivot_scaled[:, t_idx]
                elif task_type == "X":
                    chan_idx = chan_info[0]
                    ds_x[split][s_idx, chan_idx, :] = pivot_scaled[:, t_idx]
            gc.collect()
        # Holiday í”¼ì²˜ ì¶”ê°€ (ëª¨ë“  splitì— ëŒ€í•´)
        if mode == 'train':
            logger.info("5. Holiday í”¼ì²˜ ì¶”ê°€ (ì£¼ë§ í¬í•¨)")
            for i in range(total_days):
                base_date = start_date + timedelta(days=i)
                is_holiday = float(base_date.weekday() >= 5 or base_date in kr_holidays)
                hch_day = np.full((T, L), is_holiday, dtype=np.float16)
                
                day_start_idx, day_end_idx = i * T, (i + 1) * T
                for split_name, (s_start, s_end) in splits.items():
                    overlap_start, overlap_end = max(day_start_idx, s_start), min(day_end_idx, s_end)
                    if overlap_start < overlap_end:
                        write_start, write_end = overlap_start - s_start, overlap_end - s_end
                        read_start, read_end = overlap_start - day_start_idx, overlap_end - day_start_idx
                        ds_x[split_name][write_start:write_end, -1, :] = hch_day[read_start:read_end]
    

        # ì£¼ê¸°ì ìœ¼ë¡œ flush í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•˜ë©´ ë” ì•ˆì •ì ì…ë‹ˆë‹¤.
        hf_x.flush()
        if hf_y:
            hf_y.flush()

    logger.info(f"âœ… X HDF5 ìƒì„± ì™„ë£Œ: {h5_x_path}")
    if h5_y_path:
        logger.info(f"âœ… Y HDF5 ìƒì„± ì™„ë£Œ: {h5_y_path}")


def process_raw_to_cluster_tensor_pivot(
    raw_data_dir: str,
    clusters: dict[int, list[int]],
    path_config: dict,
    task_config: dict,
    output_dir: str,
    logger,
    generate_y: bool = True
):
    """pivot ê¸°ë°˜ ê³ ì† tensor ìƒì„± (train/val/test ì „ë¶€ ì²˜ë¦¬)"""
    os.makedirs(output_dir, exist_ok=True)

    # --- 1) Task ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸° ---
    interval     = task_config["interval"]         # ì˜ˆ: 5 (ë¶„)
    interest     = task_config["interest"]         # ì˜ˆ: [-12, -11, ..., 0]
    interest_y   = task_config.get("interest_y", 12)
    train_ratio  = task_config["train_ratio"]      # ì˜ˆ: (0.6,0.2,0.2)
    scaler = Scaler_tool()
    # x/y offsets (ë¶„ ë‹¨ìœ„)
    x_offsets = np.array(
        [m for base in interest for m in range(base*interval, (base+12)*interval, interval)],
        dtype=int
    )
    x_features = len(x_offsets)
    y_offsets = np.array([m*interval for m in range(interest_y)], dtype=int)
    y_features = len(y_offsets)

    # --- 2) ë§í¬ ëª©ë¡ & ë§¤í•‘ ---
    ITS_path   = path_config["data"]["ITS_info"]
    link_df    = pd.DataFrame(iter(DBF(ITS_path, encoding="cp949")))
    all_links  = link_df["LINK_ID"].to_numpy(dtype=int)
    link_df["LINK_ID"] = link_df["LINK_ID"].astype(int)
    link_to_idx= {lid: i for i, lid in enumerate(all_links)}
    max_speed_dict = dict(zip(link_df["LINK_ID"], link_df["MAX_SPD"]))

    # --- 3) í´ëŸ¬ìŠ¤í„°ë³„ ë§í¬ ì¸ë±ìŠ¤ ---
    cluster_rows = {
        cid: np.array([link_to_idx[int(l)] for l in links if int(l) in link_to_idx], dtype=int)
        for cid, links in clusters.items()
    }

    # --- 4) ë‚ ì§œ ë²”ìœ„ & split ê³„ì‚° ---
    start_date = datetime.strptime(task_config["date_range"]["start"], "%Y-%m-%d")
    end_date   = datetime.strptime(task_config["date_range"]["end"],   "%Y-%m-%d")
    total_minutes = int((end_date - start_date).total_seconds() / 60)
    total_ts      = total_minutes // interval
    train_end = int(total_ts * train_ratio[0])
    val_end   = int(total_ts * (train_ratio[0] + train_ratio[1]))
    splits = {
        "train": (0, train_end),
        "val":   (train_end, val_end),
        "test":  (val_end, total_ts),
    }
    logger.info(f"ğŸ¯ pivot ë°©ì‹: ì „ì²´ íƒ€ì„ìŠ¤íƒ¬í”„ {total_ts} (interval={interval}ë¶„)")

    # --- 5) ë‚ ì§œë³„ ì‘ì—… ë§¤í•‘ í•¨ìˆ˜ ---
    def _date_str(dt: datetime) -> str:
        return dt.strftime("%Y%m%d")
    def _slot_of(dt: datetime) -> int:
        return dt.hour * 12 + dt.minute // 5

    def build_date_tasks(split_s: int, split_e: int) -> dict[str, dict[str, list[tuple[int,int,int]]]]:
        """
        { 'YYYYMMDD': { 'x': [(rel_t, slot, k), ...], 'y': [...] }, ... }
        """
        tasks = defaultdict(lambda: {"x": [], "y": []})
        for rel_t in range(split_s, split_e):
            base_dt = start_date + timedelta(minutes=rel_t * interval)
            idx = rel_t - split_s  # dataset index 0..T-1
            # x
            for k, off in enumerate(x_offsets):
                dt_off = base_dt + timedelta(minutes=int(off))
                d = _date_str(dt_off)
                tasks[d]["x"].append((idx, _slot_of(dt_off), k))
            # y
            if generate_y:
                for k, off in enumerate(y_offsets):
                    dt_off = base_dt + timedelta(minutes=int(off))
                    d = _date_str(dt_off)
                    tasks[d]["y"].append((idx, _slot_of(dt_off), k))
        return tasks

    # --- 6) splitë³„ ì²˜ë¦¬ ---
    for split_name, (split_s, split_e) in splits.items():
        if split_s >= split_e:
            continue
        T = split_e - split_s
        logger.info(f"ğŸ”¹ {split_name} ì„¸íŠ¸ ì²˜ë¦¬: {T} ts")

        # 6-1) H5 íŒŒì¼ & ë°ì´í„°ì…‹ ìƒì„±
        h5_files, x_dsets, y_dsets = {}, {}, {}
        for cid, rows in cluster_rows.items():
            cdir = os.path.join(output_dir, str(cid))
            os.makedirs(cdir, exist_ok=True)
            h5_path = os.path.join(cdir, f"{split_name}_data.h5")
            h5_files[cid]   = h5py.File(h5_path, "w")
            x_dsets[cid]    = h5_files[cid].create_dataset("x", (T,1,len(rows),x_features), 
                                dtype="float16",    compression="lzf" ,    chunks=(1, 1, len(rows), x_features))
            if generate_y:
                y_dsets[cid] = h5_files[cid].create_dataset("y", (T,1,len(rows),y_features), 
                                dtype="float16",    compression="lzf" ,    chunks=(1, 1, len(rows), y_features))
            

        # 6-2) ë‚ ì§œâ†’ì‘ì—… ë§¤í•‘
        date_tasks = build_date_tasks(split_s, split_e)

        # 6-3) ë‚ ì§œë³„ í”¼ë²— ë¡œë“œ & ë°˜ì˜
        for dstr, task in sorted(date_tasks.items()):
            pivot = _build_pivot_for_date(
                dstr, raw_data_dir, all_links, link_to_idx, max_speed_dict
            )
            if pivot is None or not isinstance(pivot, np.ndarray):
                logger.error(f"[ERROR] Pivot failed at date {dstr}")
                continue
            pivot_sc = scaler.transform(pivot)
            del pivot
            # x process
            for rel_idx, slot, k in task["x"]:
                for cid, rows in cluster_rows.items():
                    if cid>10: continue
                    x_dsets[cid][rel_idx,0,:,k] = pivot_sc[rows, slot]
                    

            # y process
            if generate_y:
                for rel_idx, slot, k in task["y"]:
                    for cid, rows in cluster_rows.items():
                        if cid>10: continue
                        y_dsets[cid][rel_idx,0,:,k] = pivot_sc[rows, slot]

            # flush process
            for cid in cluster_rows.keys():
                if cid>10: continue
                x_dsets[cid].flush()
                if generate_y:
                    y_dsets[cid].flush()


            del pivot_sc
            gc.collect()  # ë©”ëª¨ë¦¬ ì •ë¦¬
            logger.info(f"  â–¶ {split_name} {dstr} ë°˜ì˜ ì™„ë£Œ")

        # 6-4) H5 ë‹«ê¸°
        for h5 in h5_files.values():
            h5.close()
        logger.info(f"âœ… {split_name} ì„¸íŠ¸ ì™„ë£Œ")
        

    logger.info("ğŸ‰ pivot ê¸°ë°˜ tensor ìƒì„± ì™„ë£Œ")


def process_predict_to_cluster_tensor_pivot(
    raw_data_dir: str,
    clusters: dict[int, list[int]],
    path_config: dict,
    task_config: dict,
    output_dir: str,
    logger,
):
    """time_reference ê¸°ë°˜ ê³ ì† predict ì…ë ¥ ìƒì„±

    - ê° í´ëŸ¬ìŠ¤í„° í´ë”ì— `predict_data.npy`ë¥¼ ìƒì„± (shape: [T, 1, N_links, 36])
    - ìŠ¤ì¼€ì¼ì€ í•™ìŠµ ì‹œ ì €ì¥ëœ `scaler.npy`(í´ëŸ¬ìŠ¤í„°ë³„ mean/std)ë¥¼ ì‚¬ìš©
    - ë°˜í™˜ê°’: ì˜ˆì¸¡ ê¸°ì¤€ íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸(list[str])
    """
    os.makedirs(output_dir, exist_ok=True)

    # 1) ì„¤ì • ë¡œë“œ
    interval = task_config["interval"]
    interest = task_config["interest"]
    # x ì˜¤í”„ì…‹: ë¶„ ë‹¨ìœ„
    x_offsets = np.array(
        [m for base in interest for m in range(base * interval, (base + 12) * interval, interval)],
        dtype=int,
    )
    x_features = len(x_offsets)

    # time_reference: dict ë˜ëŠ” list(dict) ëª¨ë‘ ì§€ì›
    time_ref_cfg = task_config.get("time_reference")
    if isinstance(time_ref_cfg, dict):
        time_refs = [time_ref_cfg]
    elif isinstance(time_ref_cfg, list):
        time_refs = time_ref_cfg
    else:
        raise ValueError("time_reference ì„¤ì •ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. dict ë˜ëŠ” list í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")

    # 2) ë§í¬ ëª©ë¡ & ë§¤í•‘
    ITS_path = path_config["data"]["ITS_info"]
    link_df = pd.DataFrame(iter(DBF(ITS_path, encoding="cp949")))
    all_links = link_df["LINK_ID"].to_numpy(dtype=int)
    link_df["LINK_ID"] = link_df["LINK_ID"].astype(int)
    link_to_idx = {lid: i for i, lid in enumerate(all_links)}
    max_speed_dict = dict(zip(link_df["LINK_ID"], link_df["MAX_SPD"]))

    # í´ëŸ¬ìŠ¤í„°ë³„ í–‰ ì¸ë±ìŠ¤ ì¤€ë¹„
    cluster_rows = {
        cid: np.array([link_to_idx[int(l)] for l in links if int(l) in link_to_idx], dtype=int)
        for cid, links in clusters.items()
    }

    # 3) ë„ìš°ë¯¸: ë‚ ì§œ ë¬¸ìì—´ê³¼ ìŠ¬ë¡¯ ê³„ì‚°
    def _date_str(dt: datetime) -> str:
        return dt.strftime("%Y%m%d")

    def _slot_of(dt: datetime) -> int:
        return dt.hour * 12 + dt.minute // 5

    # 4) ìƒ˜í”Œë³„ ì‘ì—… êµ¬ì„±: { 'YYYYMMDD': [('sample_idx', 'slot', 'k'), ...] }
    date_tasks = {}
    timestamps = []
    for s_idx, tr in enumerate(time_refs):
        base_date = tr.get("date")
        base_time = tr.get("time") or "00:00"
        base_dt = datetime.strptime(f"{base_date} {base_time}", "%Y-%m-%d %H:%M")
        timestamps.append(base_dt.strftime("%Y-%m-%d %H:%M"))

        for k, off in enumerate(x_offsets):
            dt_off = base_dt + timedelta(minutes=int(off))
            d = _date_str(dt_off)
            date_tasks.setdefault(d, []).append((s_idx, _slot_of(dt_off), k))

    # 5) ë‚ ì§œë³„ í”¼ë²— ë¯¸ë¦¬ ê³„ì‚° (raw â†’ pivot)
    pivot_cache = {}
    for dstr in sorted(date_tasks.keys()):
        pivot_cache[dstr] = _build_pivot_for_date(
            dstr, raw_data_dir, all_links, link_to_idx, max_speed_dict
        )

    # 6) í´ëŸ¬ìŠ¤í„°ë³„ predict_data.npy ìƒì„±
    S = len(time_refs)
    for cid, rows in cluster_rows.items():
        cdir = os.path.join(output_dir, str(cid))
        os.makedirs(cdir, exist_ok=True)

        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ (í•™ìŠµ ì‹œ ì €ì¥ë¨)
        scaler_path = os.path.join(cdir, "scaler.npy")
        if os.path.exists(scaler_path):
            scaler_obj = np.load(scaler_path, allow_pickle=True).item()
            mean = scaler_obj.get("mean", 45.0)
            std = scaler_obj.get("std", 25.0)
        else:
            logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„° {cid}: scaler.npyê°€ ì—†ì–´ ê¸°ë³¸ ìŠ¤ì¼€ì¼ ì‚¬ìš©(45/25)")
            mean, std = 45.0, 25.0
        scaler = Scaler_tool(mean, std)

        x_pred = np.zeros((S, 1, len(rows), x_features), dtype=np.float32)

        for dstr, task_list in date_tasks.items():
            pivot_raw = pivot_cache[dstr]
            if pivot_raw is None:
                logger.error(f"[ERROR] Pivot ìƒì„± ì‹¤íŒ¨: {dstr}")
                continue
            for s_idx, slot, k in task_list:
                values = pivot_raw[rows, slot]
                x_pred[s_idx, 0, :, k] = scaler.transform(values)

        save_path = os.path.join(cdir, "predict_data.npy")
        np.save(save_path, x_pred)
        logger.info(f"âœ… í´ëŸ¬ìŠ¤í„° {cid} predict_data ì €ì¥: {save_path} | shape={x_pred.shape}")

    logger.info("ğŸ‰ pivot ê¸°ë°˜ predict ì…ë ¥ ìƒì„± ì™„ë£Œ")
    return timestamps